:orphan:

:orphan:

Chapter 5: Optimal transport and statistical kernel methods
==========================================================================================

This gallery contains examples and tutorials for Chapter 5. The examples demonstrate how to use
the CodPy library for various optimal transport experiments.

Each example includes code and outputs, such as plots or results, to help you understand
how the library works.

Below are the examples for this chapter:



.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This section illustrates the application of the Linear Sum Assignment Problem (LSAP) using the CodPy library.">

.. only:: html

  .. image:: /auto_ch5/images/thumb/sphx_glr_ch5_2_thumb.png
    :alt:

  :ref:`sphx_glr_auto_ch5_ch5_2.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">5.2 Optimal Transport: LSAP (Linear Sum Assignment Problem)</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="5.3.3 Kernel Conditional Density">

.. only:: html

  .. image:: /auto_ch5/images/thumb/sphx_glr_ch5_33_thumb.png
    :alt:

  :ref:`sphx_glr_auto_ch5_ch5_33.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">5.3.3 Kernel Conditional Density</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="We benchmark the performance of different conditional density estimators: - Nadaraya–Watson (standard with bw = 13) - Nadaraya–Watson with mean scaled Matern kernel - CodPy&#x27;s projection-based ConditionerKernel">

.. only:: html

  .. image:: /auto_ch5/images/thumb/sphx_glr_ch5_34_thumb.png
    :alt:

  :ref:`sphx_glr_auto_ch5_ch5_34.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">5.3.4 Kernel Conditional Expectation</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="We start illustrating the encoding/decoding procedure using a simple interface,  which we refer to as the sampling procedure.  This procedure is designed to generate new samples that approximate the distribution  of a given dataset $Y \in \mathbb{R}^{N_y \times D_y}$  by constructing a kernel-based regressor and using a latent representation. We begin with a simple one-dimensional Monge problem to demonstrate the generative capabilities of the model.  In this experiment, we consider two types of target distributions:  a bimodal Gaussian distribution and a bimodal Student&#x27;s $t$-distribution.">

.. only:: html

  .. image:: /auto_ch5/images/thumb/sphx_glr_ch5_6_a_thumb.png
    :alt:

  :ref:`sphx_glr_auto_ch5_ch5_6_a.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">5.6.a Application of OT in Disitribution Sampling : 1D</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="5.6.b Application of OT in Disitribution Sampling : 2D">

.. only:: html

  .. image:: /auto_ch5/images/thumb/sphx_glr_ch5_6_b_thumb.png
    :alt:

  :ref:`sphx_glr_auto_ch5_ch5_6_b.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">5.6.b Application of OT in Disitribution Sampling : 2D</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="We now repeat a similar test (see 5.6a &amp; 5.6b) with a bi-modal Gaussian distribution,  in fifteen dimensions, comparing Monge and Gromov-Monge methods.  The figure plots for each of these two methods the two best and worst axis combinations,  according to the Kolmogorov-Smirnov test. As can be seen in the picture,  Gromov Wasserstein-based generative method leads to distributions  that are close to the original space, which can pass two samples tests as  Kolmogorov Smirnov ones. This property is interesting for industrial applications. ">

.. only:: html

  .. image:: /auto_ch5/images/thumb/sphx_glr_ch5_6_c_thumb.png
    :alt:

  :ref:`sphx_glr_auto_ch5_ch5_6_c.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">5.6.c Application of OT in Disitribution Sampling : High-Dimensional case</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="We demonstrate how to tackle the problem of conditional sampling using the Sampler and KernelClassifier classes from CodPy. We generate synthetic spherical data with two cluster, define a Sampler to map a latent representation to the data space, and use a KernelClassifier to assign labels to the generated data.">

.. only:: html

  .. image:: /auto_ch5/images/thumb/sphx_glr_ch5_6_d_thumb.png
    :alt:

  :ref:`sphx_glr_auto_ch5_ch5_6_d.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">5.6.d Exploration Data Analysis of the Latent Space: Spherical Data</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="In this tutorial, we introduce the problem of conditional sampling, i.e., generating samples from a distribution \( p(y|x) \) using kernel-based models.">

.. only:: html

  .. image:: /auto_ch5/images/thumb/sphx_glr_ch5_7_thumb.png
    :alt:

  :ref:`sphx_glr_auto_ch5_ch5_7.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">5.6.e Exploration Data Analysis of the Latent Space: Spherical Data - 2</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="5.7. Conditional Sampling">

.. only:: html

  .. image:: /auto_ch5/images/thumb/sphx_glr_ch5_8_thumb.png
    :alt:

  :ref:`sphx_glr_auto_ch5_ch5_8.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">5.7. Conditional Sampling</div>
    </div>


.. thumbnail-parent-div-close

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_ch5/ch5_2
   /auto_ch5/ch5_33
   /auto_ch5/ch5_34
   /auto_ch5/ch5_6_a
   /auto_ch5/ch5_6_b
   /auto_ch5/ch5_6_c
   /auto_ch5/ch5_6_d
   /auto_ch5/ch5_7
   /auto_ch5/ch5_8



.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

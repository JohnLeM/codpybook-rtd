{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 8.3 Experiments - LunarLander\nWe use the OpenAI Gym library to instanciate the gymnasium LunarLander-v3 environment and reproduce the figure from chapter 8_XXX.\n\nWe train the following agents:\n\n- PPO \n- DQN \n- Controller-based\n- Kernel Actor-Critic\n- Kernel Q-Learning\n- Kernel Q-Learning HJB\n- Kernel Policy-Gradient\n\nWe show how you can tweak some methods in each algorithm to tune them to the environment. For a detailed documentation on KAgents, see **codpy documentation**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Importing necessary modules\nimport sys\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport codpy.core as core\nimport codpy.KQLearning as KQLearning\nimport codpy.conditioning as conditioning\nfrom ignore_utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KQLearning\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class KQLearningLN(KQLearning.KQLearning):\n\n    def train(\n        self,\n        game,\n        max_training_game_size=None,\n        format=True,\n        replay_buffer=True,\n        tol=1e-2,\n        **kwargs\n    ):\n        \"\"\"\n        For LunarLander, we want to fit one kernel per game. So again, we override the train method.\n        \"\"\"\n        game = self.format(\n            game, max_training_game_size=max_training_game_size, **kwargs\n        )\n        # Here the kernel is fit on the latest game only. \n        kernel = self.optimal_states_values_function(game, verbose=True, **kwargs)\n        kernel.games = game\n        self.critic.add_kernel(kernel, **kwargs)\n        delete_kernels = []\n        for i, k in self.critic.kernels.items():\n            error = self.critic.kernels[i].bellman_error\n            if error > tol and not hasattr(self.critic.kernels[i], \"flag_kill_me\"):\n                kernel = self.optimal_states_values_function(\n                    self.critic.kernels[i].games,\n                    kernel=self.critic.kernels[i],\n                    verbose=True,\n                    **kwargs,\n                )\n                kernel.games = self.critic.kernels[i].games\n                if kernel.bellman_error >= error - tol:\n                    kernel.flag_kill_me = \"please\"\n                else:\n                    self.critic.kernels[i] = kernel\n        if (\n            len(delete_kernels) > 0\n            and len(self.critic.kernels) - len(delete_kernels) > 1\n        ):\n            new_kernels = {}\n            count = 0\n            for i in range(len(self.critic.kernels)):\n                if i not in delete_kernels:\n                    new_kernels[count] = self.critic.kernels[i]\n                    count = count + 1\n            self.critic.kernels = new_kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PolicyGradient\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PolicyGradientLN(KQLearning.PolicyGradient):\n    def train(self, game, max_training_game_size=None, **kwargs):\n        if self.actor.is_valid() and self.actor.get_x().shape[0] > self.replay_buffer.capacity:\n            return\n        params = kwargs.get(\"KCritic\", {})\n        state, action, next_state, reward, return_, done = self.format(\n            game, max_training_game_size=max_training_game_size, **kwargs\n        )\n        if len(self.replay_buffer):\n            states, actions, next_states, rewards, returns, dones = (\n                self.replay_buffer.memory\n            )\n        else:\n            states, actions, next_states, rewards, returns, dones = state, action, next_state, reward, return_, done\n            # dones[0] = True\n        games = [states, actions, next_states, rewards, returns, dones]\n\n        if self.actor.is_valid():\n            last_policy = self.actor(states)\n        else:\n            last_policy = np.full(\n                [states.shape[0], self.actions_dim], 1.0 / self.actions_dim\n            )\n        last_policy = np.where(last_policy < 1e-9, 1e-9,last_policy)\n        last_policy = np.where(last_policy > 1.-1e-9,1.- 1e-9,last_policy)\n        # update probabilities\n        if not self.actor.is_valid() or self.actor.get_x().shape[0] < self.replay_buffer.capacity:\n            advantages, self.value_function = self.get_advantages(games, policy=last_policy, **kwargs)\n            self.actor = self.update_probabilities(\n                advantages, games, last_policy=last_policy, clip=.1, **kwargs\n            )\n        else:\n            pass\n            # advantages, self.value_function = self.get_advantages(games, policy=last_policy, kernel = self.value_function,**kwargs)\n            # kernel = self.update_probabilities(\n            #     advantages, games, last_policy=last_policy,kernel = self.actor, clip=.1, **kwargs\n            # )\n        if not hasattr(self,\"scores\"):\n            self.scores = [rewards.sum()]\n        else:\n            self.scores.append(rewards.sum())\n        # if len(self.replay_buffer)+states.shape[0] < self.replay_buffer.capacity:\n        is_pushed = self.replay_buffer.push(\n            state, action, next_state, reward, return_, done, worst_game=False,**kwargs\n        )\n\n    def format(self, sarsd, max_training_game_size=None, **kwargs):\n        states, actions, next_states, rewards, dones = [\n            core.get_matrix(e) for e in sarsd\n        ]\n        actions = KQLearning.rl_hot_encoder(actions, self.actions_dim)\n        dones = core.get_matrix(dones, dtype=bool)\n        len_game=states.shape[0]\n        if max_training_game_size is not None :\n            # indices = [int(n*len_game/max_training_game_size) for n in range(0, max_training_game_size)]\n            states, actions, next_states, rewards, dones = (\n                states[-max_training_game_size:],\n                actions[-max_training_game_size:],\n                next_states[-max_training_game_size:],\n                rewards[-max_training_game_size:],\n                dones[-max_training_game_size:],\n                # states[:max_training_game_size],\n                # actions[:max_training_game_size],\n                # next_states[:max_training_game_size],\n                # rewards[:max_training_game_size],\n                # dones[:max_training_game_size],\n            )\n        returns = self.compute_returns(\n            states, actions, next_states, rewards, dones, **kwargs\n        )\n        # dones[0]=True\n        return states, actions, next_states, rewards, returns, dones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KActorCritic\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class KActorCriticLN(KQLearning.KActorCritic):\n    \"\"\"\n    Defines the main KActorCritic class.\n\n    This inherits from KQLearning.KActorCritic. You can then extend any method from the main class to fit your needs.\n\n    \"\"\"\n\n    def train(self, game, max_training_game_size=None, **kwargs):\n        if self.actor.is_valid() and self.actor.get_x().shape[0] > self.replay_buffer.capacity:\n            return\n        params = kwargs.get(\"KCritic\", {})\n        state, action, next_state, reward, return_, done = self.format(\n            game, max_training_game_size=max_training_game_size, **kwargs\n        )\n        if len(self.replay_buffer):\n            states, actions, next_states, rewards, returns, dones = (\n                self.replay_buffer.memory\n            )\n        else:\n            states, actions, next_states, rewards, returns, dones = state, action, next_state, reward, return_, done\n            # dones[0] = True\n        games = [states, actions, next_states, rewards, returns, dones]\n\n        if self.actor.is_valid():\n            last_policy = self.actor(states)\n        else:\n            last_policy = np.full(\n                [states.shape[0], self.actions_dim], 1.0 / self.actions_dim\n            )\n        last_policy = np.where(last_policy < 1e-9, 1e-9,last_policy)\n        last_policy = np.where(last_policy > 1.-1e-9,1.- 1e-9,last_policy)\n        # update probabilities\n        if not self.actor.is_valid() or self.actor.get_x().shape[0] < self.replay_buffer.capacity:\n            advantages, self.value_function = self.get_advantages(games, policy=last_policy, **kwargs)\n            self.actor = self.update_probabilities(\n                advantages, games, last_policy=last_policy, clip=.1, **kwargs\n            )\n        else:\n            pass\n            # advantages, self.value_function = self.get_advantages(games, policy=last_policy, kernel = self.value_function,**kwargs)\n            # kernel = self.update_probabilities(\n            #     advantages, games, last_policy=last_policy,kernel = self.actor, clip=.1, **kwargs\n            # )\n        if not hasattr(self,\"scores\"):\n            self.scores = [rewards.sum()]\n        else:\n            self.scores.append(rewards.sum())\n        # if len(self.replay_buffer)+states.shape[0] < self.replay_buffer.capacity:\n        is_pushed = self.replay_buffer.push(\n            state, action, next_state, reward, return_, done, worst_game=False,**kwargs\n        )\n\n    def format(self, sarsd, max_training_game_size=None, **kwargs):\n        states, actions, next_states, rewards, dones = [\n            core.get_matrix(e) for e in sarsd\n        ]\n        actions = KQLearning.rl_hot_encoder(actions, self.actions_dim)\n        dones = core.get_matrix(dones, dtype=bool)\n        len_game=states.shape[0]\n        if max_training_game_size is not None :\n            # indices = [int(n*len_game/max_training_game_size) for n in range(0, max_training_game_size)]\n            states, actions, next_states, rewards, dones = (\n                states[-max_training_game_size:],\n                actions[-max_training_game_size:],\n                next_states[-max_training_game_size:],\n                rewards[-max_training_game_size:],\n                dones[-max_training_game_size:],\n            )\n        returns = self.compute_returns(\n            states, actions, next_states, rewards, dones, **kwargs\n        )\n        # dones[0]=True\n        return states, actions, next_states, rewards, returns, dones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HJB\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class KQLearningHJBLN(KQLearning.KQLearningHJB):\n\n    def __call__(self, state, **kwargs):\n        self.eps_threshold *= 0.999\n        if np.random.random() > self.eps_threshold and self.critic.is_valid() == True:\n            z = self.all_states_actions(core.get_matrix(state).T)\n            # z = self.all_states_actions(self.get_expectation_kernel(z))\n            q_values = self.critic(z)\n            q_values += np.random.random(q_values.shape) * 1e-9\n            return np.argmax(q_values)\n        return np.random.randint(0, self.actions_dim)\n\n    def get_conditioned_kernel(self, games, **kwargs):\n        return KQLearning.get_conditioned_kernel(\n            games, base_class=conditioning.ConditionerKernel, **kwargs\n        )\n\n    def train(\n        self,\n        game,\n        max_training_game_size=None,\n        format=True,\n        replay_buffer=True,\n        tol=1e-2,\n        **kwargs\n    ):\n        # return super().train(game, max_training_game_size,format,replay_buffer, tol,**kwargs)\n        # l = len(game[0])\n        # self.gamma = np.exp(-np.log(l) / l)\n        game = self.format(\n            game, max_training_game_size=max_training_game_size, **kwargs\n        )\n        kernel = self.optimal_states_values_function(game, verbose=True, **kwargs)\n        kernel.games = game\n        # kernel.gamma = self.gamma\n        self.critic.add_kernel(kernel, **kwargs)\n        delete_kernels = []\n        for i, k in self.critic.kernels.items():\n            # self.gamma = k.gamma\n            error = self.critic.kernels[i].bellman_error\n            if error > tol and not hasattr(self.critic.kernels[i], \"flag_kill_me\"):\n                kernel = self.optimal_states_values_function(\n                    self.critic.kernels[i].games,\n                    kernel=self.critic.kernels[i],\n                    verbose=True,\n                    **kwargs,\n                )\n                kernel.games = self.critic.kernels[i].games\n                # kernel.gamma = self.critic.kernels[i].gamma\n                if kernel.bellman_error >= error - tol:\n                    # delete_kernels.append(i)\n                    kernel.flag_kill_me = \"please\"\n                else:\n                    self.critic.kernels[i] = kernel\n        if (\n            len(delete_kernels) > 0\n            and len(self.critic.kernels) - len(delete_kernels) > 1\n        ):\n            new_kernels = {}\n            count = 0\n            for i in range(len(self.critic.kernels)):\n                if i not in delete_kernels:\n                    new_kernels[count] = self.critic.kernels[i]\n                    count = count + 1\n            self.critic.kernels = new_kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KController\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class heuristic_ControllerLN:\n    \"\"\"\n    Defines the heuristic controller for LunarLander. We choose to use 12 parameters to be tweaked. \n    \"\"\"\n    dim = 12\n\n    def __init__(self, w=None, **kwargs):\n        if w is None:\n            self.w = np.ones([self.dim]) * 0.5\n        else:\n            self.w = w\n        pass\n\n    def get_distribution(self):\n        class uniform:\n            def __init__(self, shape1):\n                self.shape1 = shape1\n\n            def __call__(self, n):\n                return np.random.uniform(size=[n, self.shape1])\n\n            def support(self, v):\n                out = np.clip(v, 0, 1)\n                return out\n\n        return uniform(self.w.shape[0])\n\n    def get_thetas(self):\n        return self.w\n\n    def set_thetas(self, w):\n        self.w = w.flatten()\n\n    def __call__(self, s, **kwargs):\n        angle_targ = s[0] * self.w[0] + s[2] * self.w[1]\n        if angle_targ > self.w[2]:\n            angle_targ = self.w[2]\n        if angle_targ < -self.w[2]:\n            angle_targ = -self.w[2]\n        hover_targ = self.w[3] * np.abs(s[0])\n\n        angle_todo = (angle_targ - s[4]) * self.w[4] - (s[5]) * self.w[5]\n        hover_todo = (hover_targ - s[1]) * self.w[6] - (s[3]) * self.w[7]\n\n        if s[6] or s[7]:\n            angle_todo = self.w[8]\n            hover_todo = -(s[3]) * self.w[9]\n\n        a = 0\n        if hover_todo > np.abs(angle_todo) and hover_todo > self.w[10]:\n            a = 2\n        elif angle_todo < -self.w[11]:\n            a = 3\n        elif angle_todo > +self.w[11]:\n            a = 1\n        return a\n    \n\nclass KControllerLN(KQLearning.KController):\n    \"\"\"\n    Defines the class for optimizing the controller. \n\n    The class inherit from KQLearning.KController. You can then extend any method from the main class to fit your needs. \n\n    Parameters:\n    - state_dim: Dimension of the environment's state space.\n    - actions_dim: Dimension of the environment's action space.\n    \"\"\"\n    def __init__(self, state_dim, actions_dim, **kwargs):\n        controller = heuristic_ControllerLN(state_dim=state_dim, **kwargs)\n        super().__init__(state_dim, actions_dim, controller, **kwargs)\n\n    def get_function(self, **kwargs):\n        \"\"\"\n        The optimizer will find the best parameters which maximizes this function. \n\n        This is where you would tweak the function to be maximized.\n        \"\"\"\n        self.expectation_estimator = self.get_expectation_estimator(\n            self.x, self.y, **kwargs\n        )\n\n        def function(x):\n            expectation = self.expectation_estimator(x)\n            distance = self.expectation_estimator.distance(x)\n            return expectation + distance\n\n        return function \n\n    def format(self, sarsd, **kwargs):\n        \"\"\"\n        This formats the game data to be used in the train method\n        \"\"\"\n        state, action, next_state, reward, done = [core.get_matrix(e) for e in sarsd]\n\n        action = KQLearning.rl_hot_encoder(action, self.actions_dim)\n        action = core.get_matrix(self.controller.get_thetas()).T\n        done = core.get_matrix(done, dtype=bool)\n        return (\n            core.get_matrix(state.mean(axis=0)).T,\n            core.get_matrix(action.mean(axis=0)).T,\n            core.get_matrix(next_state.mean(axis=0)).T,\n            core.get_matrix(reward.mean(axis=0)).T,\n            core.get_matrix(done.mean(axis=0)).T,\n        )\n\ndef main():\n    # Define agents here, which will be trained in the benchmark. If game_dictionnary is empty, the benchmark will try to load data from the .pkl file\n    game_dictionary = {\n        \"PPOAgent\": PPOAgent,\n        \"Controller-based\": KControllerLN,\n        \"KACAgent\": KActorCriticLN,\n        \"PolicyGradient\": PolicyGradientLN,\n        \"DQNAgent\": DQNAgent,\n        # \"KQLearningHJBCP\": KQLearningHJBLN, #bug to solve get_transition\n        \"KQLearning\": KQLearningLN,\n    }\n\n    # Define your agent's parameters here. This dict will be passed in each agent's __init__() method.\n    extras = {\n        \"KActor\": {\n            # \"latent_shape\":[100,50],\n            \"max_size\": 1000,\n            \"n_batch\": 1000000,\n            \"max_nystrom\": 1000,\n            \"reg\": 1e-6,\n            \"order\": None,\n        },\n        \"KCritic\": {\n            \"max_size\": 1000000,\n            \"n_batch\": 1000000,\n            \"max_nystrom\": 1000,\n            \"reg\": 1e-9,\n            \"order\": None,\n        },\n        \"HJBModel\": {\n            # \"latent_shape\":[100,50],\n            \"max_size\": 100000,\n            \"n_batch\": 1000000,\n            \"max_nystrom\": 1000,\n            \"reg\": 1e-9,\n            \"order\": None,\n            \"state_dim\": 8,\n        },\n        \"Rewards\": {\n            \"max_size\": 1000000,\n            \"n_batch\": 100000,\n            \"max_nystrom\": 1000,\n            \"reg\": 1e-9,\n            \"order\": None,\n        },\n        \"NextStates\": {\n            \"max_size\": 1000000,\n            \"n_batch\": 100000,\n            \"max_nystrom\": 1000,\n            \"reg\": 1e-9,\n            \"order\": None,\n        },\n        \"DQNAgent\": {\n            # 'reward_function': mc_reward_function,\n            \"episodes\": 500,\n            \"policy_param\": 64,\n            \"target_param\": 64,\n        },\n        \"ACAgent\": {\"reward_function\": None},\n        \"QAgent\": {\n            0\n            # 'reward_function': mc_reward_function\n        },\n        \"KController\": {\n            \"reg\": 1e-9,\n            \"order\": 2,\n        },\n        \"Conditionner\": {\n            \"reg\": 1e-4,\n            \"order\": 3,\n        },\n        \"max_game\": 2000,\n        \"gamma\": 0.99,\n        \"capacity\": 10000,\n        \"max_training_game_size\": 1000,\n        # \"max_kernel\": 40\n        # \"seed\": 42,\n    }\n    seed = extras.get(\"seed\", None)\n    np.random.seed(seed)\n    softmax = lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n    test = softmax([1,0])\n    Benchmark()(\n        game_dictionary,\n        \"LunarLander-v3\",\n        num_games=100,\n        eps_threshold=0.1,\n        num_repeats=3,\n        max_time=50,\n        axis=\"episodes\",\n        # file_name=\"results_LN_final.pkl\",\n        **extras,\n    )\n    plt.show()\n    pass\n\nmain()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 6.1 Supervised learning: reproducibility illustration with housing prices\n\nWe show how to reproduce the results of the chapter 6.2.1 - Application to supervised machine learning - Regression problem: housing price prediction of the book.\nWe will compare the codpy model with other standard regression methods.\nThe goal is to show the codpy model capacity to fit the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Necessary Imports\n ------------------------\n########################################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\nimport os \nimport time\nfrom pathlib import Path\n\ntry:\n    base_path = Path(__file__).parent.parent\nexcept NameError:\n    base_path = Path.cwd().parent\n\nsys.path.append(str(base_path))\n\ntry:\n    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\nexcept NameError:\n    CURRENT_DIR = os.getcwd()\ndata_path = os.path.join(CURRENT_DIR, \"data\")\nPARENT_DIR = os.path.abspath(os.path.join(CURRENT_DIR, \"..\"))\nsys.path.insert(0, PARENT_DIR)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom utils.ch9.path_generation import California_data_generator\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nfrom codpy.kernel import Kernel\nfrom codpy.plot_utils import multi_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regression models\nIn the following methods we define the regression models to be used for the comparison.\nEach model gets evaluated on the training data based on the Mean Squared Error (MSE) loss.\nWe also compute the Maximum Mean Discrepancy (MMD) for the codpy model, used to showcase the direct inverse relationship between MMD and Score.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use CodPy for regression, we instanciate a Kernel object\nand pass x as the training data and fx as the target values.\nCalling the kernel with z will return the predictions,\nsimilar to how we would use predict().\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def codpy_model(x, fx, z, fz):\n    kernel = Kernel(\n        x=x,\n        fx=fx,\n    )\n    results = kernel(z)\n    eval_loss = np.mean((results - fz) ** 2)\n    mmd = np.sqrt(kernel.discrepancy(z))\n    return eval_loss, mmd\n\n\n# Different other standard models are defined below.\n# The user can add these models to the lists of models to be evaluated.\ndef torch_model(x, fx, z, fz):\n    input_scaler = StandardScaler()\n    x = input_scaler.fit_transform(x)\n    z = input_scaler.transform(z)\n\n    target_scaler = StandardScaler()\n    fx = target_scaler.fit_transform(fx.reshape(-1, 1)).reshape(-1)\n    fz_scaled = target_scaler.transform(fz.reshape(-1, 1)).reshape(-1)\n\n    x = torch.tensor(x, dtype=torch.float32)\n    fx = torch.tensor(fx, dtype=torch.float32)\n    z = torch.tensor(z, dtype=torch.float32)\n    fz = torch.tensor(fz_scaled, dtype=torch.float32)\n\n    class FFN(nn.Module):\n        def __init__(self, input_dim):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 64),\n                nn.ReLU(),\n                nn.Linear(64, 128),\n                nn.ReLU(),\n                nn.Linear(128, 64),\n                nn.ReLU(),\n                nn.Linear(64, 1),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    model = FFN(x.shape[1])\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n    loss_fn = nn.MSELoss()\n\n    n_samples = x.shape[0]\n    batch_size = max(n_samples // 4, 32)\n\n    start_time = time.perf_counter()\n\n    model.train()\n    for epoch in range(128):\n        indices = torch.randperm(n_samples)\n        for start in range(0, n_samples, batch_size):\n            end = min(start + batch_size, n_samples)\n            batch_idx = indices[start:end]\n            x_batch = x[batch_idx]\n            fx_batch = fx[batch_idx]\n\n            pred = model(x_batch).squeeze()\n            loss = loss_fn(pred, fx_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        pred_scaled = model(z).squeeze().numpy()\n        pred = target_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).reshape(-1)\n        true = target_scaler.inverse_transform(fz.numpy().reshape(-1, 1)).reshape(-1)\n        eval_loss = np.mean((pred - true) ** 2)\n\n    return eval_loss, None\n\n\ndef random_forest_model(x, fx, z, fz):\n    x, fx, z, fz = np.array(x), np.array(fx).ravel(), np.array(z), np.array(fz).ravel()\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\n    model.fit(x, fx)\n    results = model.predict(z)\n    eval_loss = np.mean((results - fz) ** 2)\n    return eval_loss, None\n\n\ndef adaboost_model(x, fx, z, fz):\n    x, fx, z, fz = np.array(x), np.array(fx).ravel(), np.array(z), np.array(fz).ravel()\n    from sklearn.ensemble import AdaBoostRegressor\n\n    model = AdaBoostRegressor(n_estimators=50, learning_rate=1)\n    model.fit(x, fx)\n    results = model.predict(z)\n    eval_loss = np.mean((results - fz) ** 2)\n    return eval_loss, None\n\n\ndef xgboost_model(x, fx, z, fz):\n    x, fx, z, fz = np.array(x), np.array(fx).ravel(), np.array(z), np.array(fz).ravel()\n    import xgboost as xgb\n\n    model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1)\n    model.fit(x, fx)\n    results = model.predict(z)\n    eval_loss = np.mean((results - fz) ** 2)\n    return eval_loss, None\n\n\ndef svr_model(x, fx, z, fz):\n    x, fx, z, fz = np.array(x), np.array(fx).ravel(), np.array(z), np.array(fz).ravel()\n    from sklearn.svm import SVR\n\n    model = SVR(kernel=\"rbf\", C=1.0, epsilon=0.1)\n    model.fit(x, fx)\n    results = model.predict(z)\n    eval_loss = np.mean((results - fz) ** 2)\n    return eval_loss, None\n\n\ndef gradient_boosting_model(x, fx, z, fz):\n    x, fx, z, fz = np.array(x), np.array(fx).ravel(), np.array(z), np.array(fz).ravel()\n    from sklearn.ensemble import GradientBoostingRegressor\n\n    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n    model.fit(x, fx)\n    results = model.predict(z)\n    eval_loss = np.mean((results - fz) ** 2)\n    return eval_loss, None\n\n\ndef decision_tree_model(x, fx, z, fz):\n    x, fx, z, fz = np.array(x), np.array(fx).ravel(), np.array(z), np.array(fz).ravel()\n    from sklearn.tree import DecisionTreeRegressor\n\n    model = DecisionTreeRegressor(max_depth=5)\n    model.fit(x, fx)\n    results = model.predict(z)\n    eval_loss = np.mean((results - fz) ** 2)\n    return eval_loss, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gathering Results\nHere we benchmark 3 of the models defined above.\nWe use the California housing dataset, and train the models on different subsets of the training dataset.\nWe always evaluate the models on the entire training set. Therefore, the last training procedure trains and evaluate on the exact same data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = {}\ntimes = {}\nmmds = {}\nmodels = [torch_model, random_forest_model, codpy_model]\nmodel_aliases = {\n    \"torch_model\": \"FFN\",\n    \"codpy_model\": \"KRR\",\n    \"random_forest_model\": \"RF\",\n}\ndata_generator_ = California_data_generator()\nX, FX, X, FX, Z, FZ = data_generator_.get_data(-1, -1, -1, -1)\ntorch.manual_seed(0)\nidxs = torch.randperm(len(X))\nSIZE = 512\nX, FX, Z, FZ = (\n    X.values[idxs][:SIZE],\n    FX.values[idxs][:SIZE],\n    Z.values[idxs][:SIZE],\n    FZ.values[idxs][:SIZE],\n)\nlength_ = len(X)\n# Different slices of the data to be used for x different training & evaluation procedures\nscenarios_list = [\n    (int(i), int(i), -1, -1) for i in np.arange(16, length_ + 1, (length_ - 16) / 10)\n]\nfor scenario in scenarios_list:\n    Nx, Nfx, Nz, Nfz = scenario\n    x, fx, z, fz = X[:Nx], FX[:Nfx], Z[:Nz], FZ[:Nfz]\n    results[len(x)] = {}\n    times[len(x)] = {}\n    mmds[len(x)] = {}\n    for model in models:\n        start_time = time.perf_counter()\n        loss, mmd = model(x, fx, z, fz)\n        mmds[len(x)][model.__name__] = mmd\n        results[len(x)][model.__name__] = loss\n        end_time = time.perf_counter()\n        times[len(x)][model.__name__] = end_time - start_time\n        print(\n            f\"Model: {model.__name__}, Time taken: {times[len(x)][model.__name__]:.4f} seconds\"\n        )\nres = [{\"data\": results}, {\"data\": mmds}, {\"data\": times}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_one(inputs):\n    results = inputs[\"data\"]\n    ax = inputs[\"ax\"]\n    legend = inputs[\"legend\"]\n\n    model_aliases = {\n        \"torch_model\": \"FFN\",\n        \"codpy_model\": \"KRR\",\n        \"random_forest_model\": \"RF\",\n    }\n\n    x_vals = sorted(results.keys())\n    for model_name in next(iter(results.values())).keys():\n        y_vals = [results[x][model_name] for x in x_vals]\n        label = model_aliases.get(model_name, model_name)\n        ax.plot(x_vals, y_vals, marker=\"o\", label=label)\n\n    ax.set_xlabel(\"Number of Training Examples\")\n    ax.set_ylabel(legend)\n    ax.legend()\n    ax.grid(True)\n\n    return ax\n\n\nmulti_plot(\n    res,\n    plot_one,\n    mp_nrows=1,\n    mp_ncols=3,\n    legends=[\"MSE\", \"MMD\", \"Times\"],\n    mp_figsize=(14, 10),\n)\nplt.show()\npass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
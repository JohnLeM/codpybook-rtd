{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 6.3 Unsupervised learning: Clustering - MNIST\n\nWe show how to reproduce the results of the chapter 6.3.2 - Application to supervised machine learning - Classification problem: handwritten digits of the book.\nWe will compare the codpy MMD minimization-based algorithm with scikit learn k-means in an unsupervised setting.\nThe goal is to show the different scores as we increase the number of centroids Ny used for clustering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Necessary Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport time\n\nos.environ[\"OPENBLAS_NUM_THREADS\"] = \"32\"\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\n\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom ch6_Clustering import *\nfrom codpy.kernel import *\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix, pairwise_distances\nfrom torchvision import datasets, transforms\n\ntry:\n    current_dir = os.path.dirname(__file__)\n    data_dir = os.path.join(current_dir, \"data\")\nexcept NameError:\n    current_dir = os.getcwd()\n    data_dir = os.path.join(current_dir, \"data\")\n\ncurr_f = os.path.join(os.getcwd(), \"codpy-book\", \"utils\")\nsys.path.insert(0, curr_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MNIST Data Preparation\nWe normalize pixel values and reshape data for processing.\nPixel data is used as flat vectors, and labels are one-hot encoded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_MNIST_data(N=-1, flatten=True, one_hot=True, seed=43):\n    transform = transforms.ToTensor()\n\n    train_data = datasets.MNIST(\n        root=\".\", train=True, download=True, transform=transform\n    )\n    test_data = datasets.MNIST(\n        root=\".\", train=False, download=True, transform=transform\n    )\n\n    x = train_data.data.numpy().astype(np.float32) / 255.0\n    fx = train_data.targets.numpy()\n\n    z = test_data.data.numpy().astype(np.float32) / 255.0\n    fz = test_data.targets.numpy()\n\n    if flatten:\n        x = x.reshape(len(x), -1)\n        z = z.reshape(len(z), -1)\n\n    if one_hot:\n        fx = np.eye(10)[fx]\n        fz = np.eye(10)[fz]\n    if N == -1:\n        N = x.shape[0]\n    indices = random.sample(range(x.shape[0]), min(N, x.shape[0]))\n    x, fx = x[indices], fx[indices]\n\n    return x, fx, z, fz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering Models\nThis section defines the K-means and CodPy clustering models.\nWe wrap the clustering methods with kernels assigning labels to clusters based on the target values `fx`.\nThis allow us to compute score metrics for the models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def codpy_sharp(x, fx, Ny):\n    # Select the codpy model for clustering and select centers\n    greedy_search = SharpDiscrepancy(x=x, N=Ny)\n    centers = greedy_search.cluster_centers_\n    # Set a classifier which will assign labels to clusters based on fx\n    kernel = KernelClassifier(x=x, y=centers, fx=fx)\n    kernel.set_kernel_ptr(greedy_search.k.kernel)\n    return centers, kernel\n\n\ndef codpy_clustering(x, fx, Ny):\n    # Select the codpy model for clustering and select centers\n    greedy_search = GreedySearch(x=x, N=Ny)\n    centers = greedy_search.cluster_centers_\n    # Set a classifier which will assign labels to clusters based on fx\n    kernel = KernelClassifier(x=x, y=centers, fx=fx)\n    kernel.set_kernel_ptr(greedy_search.k.kernel)\n    return centers, kernel\n\n\ndef kmeans_clustering(x, fx, Ny):\n    # Use Kmeans from sklearn to get the clusters\n    if Ny >= x.shape[0]:\n        centers = x\n    else:\n        centers = KMeans(n_clusters=Ny, random_state=1).fit(x).cluster_centers_\n    # Set a classifier which will assign labels to centers based on fx\n    kernel = KernelClassifier(x=x, y=centers, fx=fx)\n    return centers, kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the Experiment\nThis section runs the experiment to compare K-means and CodPy clustering.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def one_experiment(X, fx, Ny, get_predictor, z, fz):\n    def get_score(X, cluster_centers, predictor):\n        inertia = compute_inertia(X, cluster_centers)\n        mmd = compute_mmd(X, cluster_centers)\n\n        # The score is accuracy\n        f_z = predictor(z)\n        f_z = f_z.argmax(1)\n        ground_truth = fz.argmax(axis=-1)\n        out = confusion_matrix(ground_truth, f_z)\n        score = np.trace(out) / np.sum(out)\n\n        return inertia, mmd, score\n\n    elapsed_time = time.time()\n    cluster_centers, predictor = get_predictor(X, fx, Ny)\n    elapsed_time = time.time() - elapsed_time\n    inertia, mmd, score = get_score(X, cluster_centers, predictor)\n    return inertia, mmd, elapsed_time, score\n\n\ndef run_experiment(\n    data_generator, Nx, Ny_values, get_predictors, labels, file_name=None\n):\n    results = []\n    for Ny in Ny_values:\n        N_MNIST_pics = Nx\n        x, fx, z, fz = data_generator(N_MNIST_pics)\n        for get_predictor, label in zip(get_predictors, labels):\n            inertia, mmd, elapsed_time, score = one_experiment(\n                x, fx, Ny, get_predictor, z, fz\n            )\n            print(\n                \"Method:\",\n                label,\n                \"N_partition:\",\n                Ny,\n                \"inertia:\",\n                inertia,\n                \"mmd:\",\n                mmd,\n                \"time:\",\n                elapsed_time,\n                \"score\",\n                score,\n            )\n            results.append(\n                {\n                    \"Method\": label,\n                    \"Nx\": Nx,\n                    \"Ny\": Ny,\n                    \"Execution Time (s)\": elapsed_time,\n                    \"inertia\": inertia,\n                    \"mmd\": mmd,\n                    \"score\": score,\n                }\n            )\n    out = pd.DataFrame(results)\n    print(out)\n    if file_name is not None:\n        out.to_csv(file_name, index=False)\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting\nThis section formats data plots the different experiments on a figure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_experiment(inputs):\n    \"\"\"\n    This is mainly boilerplate formatting the data for plotting.\n    \"\"\"\n    results = [{\"data\": {}} for _ in range(4)]\n    for res in inputs:\n        ny = res[\"Ny\"]\n        method = res[\"Method\"]\n        t = res[\"Execution Time (s)\"]\n        inertia = res[\"inertia\"]\n        mmd = res[\"mmd\"]\n        score = res[\"score\"]\n        results[0][\"data\"].setdefault(ny, {})[method] = score\n        results[1][\"data\"].setdefault(ny, {})[method] = mmd\n        results[2][\"data\"].setdefault(ny, {})[method] = inertia\n        results[3][\"data\"].setdefault(ny, {})[method] = t\n\n    def plot_one(inputs):\n        results = inputs[\"data\"]\n        ax = inputs[\"ax\"]\n        legend = inputs[\"legend\"]\n        for model_name in next(iter(results.values())).keys():\n            x_vals = sorted(results.keys())\n            y_vals = [results[x][model_name] for x in x_vals]\n            ax.plot(x_vals, y_vals, marker=\"o\", label=model_name)\n        ax.set_xlabel(\"Ny\")\n        ax.set_ylabel(legend)\n        ax.legend()\n        ax.grid(True)\n\n        return ax\n\n    multi_plot(\n        results,\n        plot_one,\n        mp_nrows=1,\n        mp_ncols=4,\n        mp_figsize=(14, 10),\n        legends=[\"Scores\", \"discrepancy_errors\", \"inertia\", \"execution_time\"],\n    )\n\n\nget_predictors = [\n    lambda X, fx, N: codpy_sharp(X, fx, N),\n    lambda X, fx, N: codpy_clustering(X, fx, N),\n    lambda X, fx, N: kmeans_clustering(X, fx, N),\n]\nlabels = [\"sharp\", \"greedy\", \"kmeans\"]\nfile_name = [\"clustering.csv\"]\nNxs, Nys = 2**14, [10, 20, 40, 80, 160]\nfile_name = os.path.join(data_dir, \"clusteringMNIST.csv\")\nresults = run_experiment(\n    get_MNIST_data, Nxs, Nys, get_predictors, labels, file_name=file_name\n)\nplot_experiment(results)\nplt.show()\npass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
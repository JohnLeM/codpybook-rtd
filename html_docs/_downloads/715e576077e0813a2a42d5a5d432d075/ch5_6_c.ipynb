{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 5.6.c Application of OT in Disitribution Sampling : High-Dimensional case\n\nWe now repeat a similar test (see 5.6a & 5.6b) with a bi-modal Gaussian distribution, \nin fifteen dimensions, comparing Monge and Gromov-Monge methods. \nThe figure plots for each of these two methods the two best and worst axis combinations, \naccording to the Kolmogorov-Smirnov test. As can be seen in the picture, \nGromov Wasserstein-based generative method leads to distributions \nthat are close to the original space, which can pass two samples tests as \nKolmogorov Smirnov ones. This property is interesting for industrial applications. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nfrom codpy import core\nfrom codpy.kernel import Kernel,Sampler\nfrom codpy.plot_utils import multi_plot\n\n\ndef normal_wrapper(center, size, radius=1.0, **kwargs):\n    return np.random.normal(loc=center, scale=radius, size=size)\n\n\ndef student_wrapper(center, size, **kwargs):\n    df = kwargs.get(\"df\", 3.0)\n    out = np.random.standard_t(df, size=size)\n    out += center\n    return out\n\n\ndef generate_multimodal_data(\n    N=500,\n    D=1,\n    num_clusters=2,\n    centers=None,\n    radii=None,\n    weights=None,\n    random_variable=None,\n    seed=None,\n    **kwargs,\n):\n    \"\"\"\n    Generate synthetic multimodal data from a mixture of clusters.\n\n    Parameters:\n        N (int): Total number of samples.\n        D (int): Dimensionality of the data.\n        num_clusters (int): Number of clusters.\n        centers (np.ndarray): Optional. Shape (num_clusters, D).\n        radii (np.ndarray): Optional. Std dev per cluster.\n        weights (np.ndarray): Optional. Cluster weights (should sum to 1).\n        random_variable (callable): Custom sampling function. Default is np.random.normal.\n\n    Returns:\n        x (pd.DataFrame): Data samples.\n        labels (pd.Series): Cluster labels for each sample.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)    \n    if centers is None:\n        centers = np.random.normal(loc=0.0, scale=0.5, size=(num_clusters, D))\n        centers -= centers.mean(axis=0)\n        centers = centers * 4./np.linalg.norm(centers, axis=1, keepdims=True)\n\n    if radii is None:\n        radii = np.abs(np.random.normal(loc=0.2, scale=0.1, size=num_clusters))\n\n    if weights is None:\n        weights = np.ones(num_clusters) / num_clusters\n\n    if random_variable is None:\n        random_variable = normal_wrapper\n\n    x_list, label_list = [], []\n\n    for i in range(num_clusters):\n        num_samples = int(N * weights[i])\n        samples = random_variable(\n            center=centers[i], size=(num_samples, D), radius=radii[i], **kwargs\n        )\n        x_list.append(samples)\n        label_list.extend([i] * num_samples)\n\n    x = pd.DataFrame(np.vstack(x_list), columns=[f\"dim_{d}\" for d in range(D)])\n    labels = pd.Series(label_list, name=\"cluster\")\n    return x, labels\n\n\ndef df_summary(df):\n    return pd.DataFrame(\n        {\n            \"Mean\": df.mean(),\n            \"Variance\": df.var(),\n            \"Skewness\": df.skew(),\n            \"Kurtosis\": df.kurtosis(),\n        }\n    )\n\n\nfrom scipy.stats import ks_2samp\n\n\ndef ks_testD(x, y, alpha=0.05):\n    \"\"\"\n    Performs Kolmogorov-Smirnov test for each dimension.\n\n    Parameters:\n        x (np.ndarray or pd.DataFrame): First sample.\n        y (np.ndarray or pd.DataFrame): Second sample.\n        alpha (float): Significance level (default 0.05).\n\n    Returns:\n        pd.Series: p-values from the KS test.\n        pd.Series: Constant threshold values (same for all dimensions).\n    \"\"\"\n    x = x.values if isinstance(x, pd.DataFrame) else x\n    y = y.values if isinstance(y, pd.DataFrame) else y\n\n    D = x.shape[1]\n    p_values = []\n    thresholds = []\n\n    for i in range(D):\n        stat = ks_2samp(x[:, i], y[:, i])\n        p_values.append(stat.pvalue)\n        thresholds.append(alpha)  # Optional: could vary if computed per dim\n\n    return pd.Series(p_values, name=\"p-value\"), pd.Series(thresholds, name=\"threshold\")\n\n\ndef stats_df(dfx_list, dfy_list, f_names=None, fmt=\"{:.2g}\"):\n    \"\"\"\n    Computes and formats summary statistics between reference and sampled data.\n\n    Parameters:\n        dfx_list (list): List of reference datasets (np.ndarray or pd.DataFrame).\n        dfy_list (list): List of sampled datasets (np.ndarray or pd.DataFrame).\n        f_names (list): Optional. Row labels. Should match total number of columns across all datasets.\n        fmt (str): Format string for floats.\n\n    Returns:\n        pd.DataFrame: Formatted summary statistics.\n    \"\"\"\n\n    if not isinstance(dfx_list, list):\n        dfx_list = [dfx_list]\n    if not isinstance(dfy_list, list):\n        dfy_list = [dfy_list]\n\n    def format_pair(x_vals, y_vals):\n        return [f\"{fmt.format(x)} ({fmt.format(y)})\" for x, y in zip(x_vals, y_vals)]\n\n    all_stats, full_index = [], []\n    for i, (dfx, dfy) in enumerate(zip(dfx_list, dfy_list)):\n        dfx = pd.DataFrame(dfx)\n        dfy = pd.DataFrame(dfy)\n\n        sx, sy = df_summary(dfx), df_summary(dfy)\n        ks_df, ks_thr = ks_testD(dfx, dfy)\n\n        stats = {\n            \"Mean\": format_pair(sx.Mean, sy.Mean),\n            \"Variance\": format_pair(sx.Variance, sy.Variance),\n            \"Skewness\": format_pair(sx.Skewness, sy.Skewness),\n            \"Kurtosis\": format_pair(sx.Kurtosis, sy.Kurtosis),\n            \"KS test\": format_pair(ks_df, ks_thr),\n        }\n\n        all_stats.append(pd.DataFrame(stats, index=dfx.columns))\n        if f_names and i < len(f_names):\n            full_index.extend([f\"{f_names[i]}:{col}\" for col in dfx.columns])\n        else:\n            full_index.extend(dfx.columns)\n\n    result = pd.concat(all_stats)\n    result.index = full_index\n    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "High dimensional illustrations\n\n########################################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def D_dim_table(f_x, f_z, f_names=[\"OT\", \"with encoding\"]):\n    \"\"\"\n    Computes rows with Max, Median, and Min KS test values for both methods.\n\n    Parameters:\n        f_x, f_z: Tuple of (true_data, generated_data) for two methods.\n        f_names (list): Labels for the two methods.\n\n    Returns:\n        pd.DataFrame: Summary table with labeled rows.\n    \"\"\"\n\n    def extract_extremes(df_stats):\n        # Parse KS values\n        ks_vals = df_stats[\"KS test\"].str.split(\"(\", expand=True)[0].astype(float)\n\n        # Identify indices for extreme cases\n        idx_max = ks_vals.idxmax()\n        idx_median = (ks_vals - ks_vals.median()).abs().idxmin()\n        idx_min = ks_vals.idxmin()\n\n        return df_stats.loc[[idx_max, idx_median, idx_min]]\n\n    # Get individual stats tables\n    table1 = stats_df(f_x[0], f_x[1])\n    table2 = stats_df(f_z[0], f_z[1])\n\n    # Extract key rows\n    summary1 = extract_extremes(table1)\n    summary2 = extract_extremes(table2)\n\n    # Assign readable row labels\n    summary1.index = [\n        f\"{f_names[0]} (Max)\",\n        f\"{f_names[0]} (Median)\",\n        f\"{f_names[0]} (Min)\",\n    ]\n    summary2.index = [\n        f\"{f_names[1]} (Max)\",\n        f\"{f_names[1]} (Median)\",\n        f\"{f_names[1]} (Min)\",\n    ]\n\n    return pd.concat([summary1, summary2])\n\n\ndef D_dim_index(f_x, f_z, f_names=[\"OT\", \"with encoding\"]):\n    \"\"\"\n    Identifies indices with lowest and highest KS test scores across dimensions.\n\n    Returns:\n        (list, list): (min_indices, max_indices)\n    \"\"\"\n    min_indices = []\n    max_indices = []\n\n    for x, z in zip(f_x, f_z):\n        df = stats_df(x, z)\n\n        ks_vals = df[\"KS test\"].str.split(\"(\", expand=True)[0].astype(float)\n        min_indices.extend(ks_vals.nsmallest(2).index.tolist())\n        max_indices.extend(ks_vals.nlargest(2).index.tolist())\n\n    return min_indices, max_indices\n\n\ndef scatter_plot(xfx, ax=None, title=\"\", **kwargs):\n    xp, fxp = xfx[0], xfx[1]\n    ax.scatter(xp[:, 0], xp[:, 1], label=\"True\", alpha=0.5)\n    ax.scatter(fxp[:, 0], xp[:, 1], label=\"Generated\", alpha=0.5)\n    ax.set_title(title)\n    ax.legend()\n\n\ndef plot_best_worst(true_data, generated_data):\n    \"\"\"\n    Plots best and worst reconstructed dimensions based on KS statistics.\n    \"\"\"\n    min_indices, max_indices = D_dim_index(true_data, generated_data)\n\n    def extract_by_index(data, indices):\n        return [\n            d[:, [i for i in indices if isinstance(i, str) or i in range(d.shape[1])]]\n            for d in data\n        ]\n\n    best_OT = extract_by_index([true_data[0], generated_data[0]], max_indices)\n    best_enc = extract_by_index([true_data[1], generated_data[1]], max_indices)\n    worst_OT = extract_by_index([true_data[0], generated_data[0]], min_indices)\n    worst_enc = extract_by_index([true_data[1], generated_data[1]], min_indices)\n\n    plot_data = [best_OT, best_enc, worst_OT, worst_enc]\n    titles = [\"Monge best\", \"Gromov-Monge best\", \"Monge worst\", \"Gromov-Monge worst\"]\n\n    multi_plot(\n        plot_data,\n        fun_plot=scatter_plot,\n        f_names=titles,\n        mp_nrows=1,\n        mp_figsize=(12, 4),\n        legends=[[\"True\", \"Generated\"]] * len(plot_data),\n    )\n\n\ndef get_figures_data(\n    D=2, N=300, N_samples=300, centers=None, random_variable=normal_wrapper, **kwargs\n):\n    \"\"\"\n    Generates synthetic multimodal data and samples from it using kernel-based mapping.\n\n    Parameters:\n        D (int): Dimension of the data.\n        N (int): Number of reference samples.\n        N_samples (int): Number of samples to generate from the learned sampler.\n        centers (np.ndarray): Optional custom centers for the clusters.\n        random_variable (callable): Sampling function (e.g., normal_wrapper or student_wrapper).\n\n    Returns:\n        tuple: (reference_data (np.ndarray), sampled_data (np.ndarray))\n    \"\"\"\n    # Default centers: two symmetric clusters\n    if centers is None:\n        centers = np.vstack([np.full(D, 3.0), np.full(D, -3.0)])\n\n    # Generate reference multimodal dataset\n    ref_df, _ = generate_multimodal_data(\n        N=N, D=D, centers=centers, random_variable=random_variable, **kwargs\n    )\n    ref = ref_df.values\n\n    # Sample from it using kernel + sampler\n    sampler = Sampler(ref,**kwargs)\n    sampled = sampler.sample(N_samples)\n\n    return ref, sampled\n\n\ndef figure(\n    xy,\n    f_names=None,\n    mp_ncols=2,\n    mp_nrows=1,\n    fun_plot=scatter_plot,\n    figsize=(10, 4),\n    legends=None,\n    **kwargs,\n):\n    \"\"\"\n    Displays multiple 2D plots of true vs generated data.\n\n    Parameters:\n        xy (list): List of (reference, sampled) pairs.\n        f_names (list): List of titles for each subplot.\n        mp_ncols (int): Number of columns in the plot grid.\n        mp_nrows (int): Number of rows in the plot grid.\n        fun_plot (callable): Plotting function (default: scatter_plot).\n        figsize (tuple): Figure size for the whole plot.\n        legends (list): Custom legends per subplot.\n        **kwargs: Additional keyword arguments for plotting function.\n    \"\"\"\n    # Default names if none provided\n    if f_names is None:\n        f_names = [f\"Figure {i+1}\" for i in range(len(xy))]\n\n    # Default legends\n    if legends is None:\n        legends = [[\"True\", \"Generated\"]] * len(xy)\n\n    multi_plot(\n        xy,\n        fun_plot=fun_plot,\n        f_names=f_names,\n        mp_ncols=mp_ncols,\n        mp_nrows=mp_nrows,\n        mp_figsize=figsize,\n        legends=legends,\n        **kwargs,\n    )\n\n\ndef label413(\n    kwargs=None,\n    encode_d=1,\n    f_names=[\"Monge\", \"Gromov-Monge\"],\n    fun_plot=scatter_plot,\n):\n    \"\"\"\n    Compare performance of OT vs Parametric encoder over high-dimensional data (D=15).\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    # Generate data (normal for both, but one with encoder Dx=1)\n    out_ot = get_figures_data(dist=normal_wrapper, Nz=300, D=15, seed=42,**kwargs)\n    latent_generator = lambda n: np.array(range(n))/n\n    out_enc = get_figures_data(dist=normal_wrapper, Nz=300, D=15, latent_dim=1,reg=0.,latent_generator=latent_generator,seed=42, **kwargs)\n\n    # Summary table\n    summary_table = D_dim_table(out_ot, out_enc, f_names=f_names)\n\n    # Show plots\n    figure([out_ot, out_enc], f_names=f_names, fun_plot=fun_plot, **kwargs)\n    plot_best_worst([out_ot[0], out_enc[0]], [out_ot[1], out_enc[1]])\n\n    return summary_table\n\n# core.KerInterface.set_verbose()\nprint(label413())\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
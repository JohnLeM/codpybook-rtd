{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 5.3.4 Kernel Conditional Expectation\n\nWe benchmark the performance of different conditional density estimators:\n- Nadaraya\u2013Watson (standard with bw = 13)\n- Nadaraya\u2013Watson with mean scaled Matern kernel\n- CodPy's projection-based ConditionerKernel\n\nWe generate synthetic data with a nonlinear conditional mean and heteroscedastic variance.\n\nMathematically, the data is generated as:\n\n$$\nX \\sim \\mathcal{U}(-1, 1), \\quad Y \\mid X = x \\sim \\mathcal{N}\\big( \\mu(x), \\sigma^2(x) \\big)\n$$\n\nwhere:\n\n- Conditional mean: $\\mu(x) = \\cos(2\\pi x)$\n- Conditional standard deviation: $\\sigma(x) = 0.1 \\cdot \\cos\\left( \\frac{\\pi x}{2} \\right)$\n\nThis allows us to test both the ability to recover nonlinear trends in the conditional mean\nand to handle non-constant conditional variance.\n\nWe compare estimated conditional expectations $\\mathbb{E}[Y \\mid X=x]$ with the ground truth\n$\\mu(x)$ across a grid of evaluation points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Required imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\nfrom codpy.conditioning import ConditionerKernel, NadarayaWatsonKernel\nfrom codpy.core import kernel_setter\nimport codpy.core \n\ncodpy.core.KerInterface.set_verbose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating the data\nThe data is generated according to the chapter 5.3.3 of the book. - Illustrative example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def mean_function_hard(x):\n    return np.cos(2 * np.pi * x)\n\n\ndef variance_function(x):\n    return 0.1 * np.cos(np.pi * x * 0.5)\n\n\ndef mean_function(x):\n    return np.sin(np.pi * x)\n\n\ndef variance_function_hard(X):\n    return 0.3 - 0.1 * mean_function(X)\n\n\ndef generate_conditional_hetero_skedastic_density_data(\n    N_train=1000, seed=None, mean_f=None, variance_f=None\n):\n    \"\"\"\n    Generate synthetic data with nonlinear and heteroscedastic structure.\n\n    Parameters:\n    - N_train: Number of training samples (X, Y)\n    - seed: Random seed for reproducibility\n    - mean_f: Callable for mean function, e.g., mean_function(x)\n    - variance_f: Callable for variance function, e.g., variance_function(x)\n\n    Returns:\n    - x: Training input samples (1D array)\n    - y: Training output samples (1D array)\n    - z: Test input sample (scalar)\n    - y_pdf: Grid of y values for PDF estimation\n    - (fz_mean, fz_std): True conditional mean and std at z\n    - density: True conditional density at z over y_pdf\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    if mean_f is None:\n        mean_f = mean_function\n    if variance_f is None:\n        variance_f = variance_function\n\n    x = np.random.uniform(-1.0, 1.0, N_train)\n\n    mean_y = mean_f(x)\n    std_y = variance_f(x)\n\n    y = np.random.normal(loc=mean_y, scale=std_y)\n\n    z = 0.0\n    fz_mean = mean_f(z)\n    fz_std = variance_f(z)\n\n    y_pdf = np.linspace(2.0 * y.min(), 2.0 * y.max(), N_train)\n    density = norm.pdf(y_pdf, loc=fz_mean, scale=fz_std)\n    density /= density.sum(axis=0)\n\n    return x, y, z, y_pdf, (fz_mean, fz_std), density"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full curve: conditional mean across a grid\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 1, figsize=(14, 5))\nX_eval = np.linspace(-1, 1, 300)\n\n# Case 1 (Smooth, theta = \\pi/2)\nx, y, z, y_pdf, (fz_mean, fz_std), density = (\n    generate_conditional_hetero_skedastic_density_data(\n        N_train=1000,\n        seed=3,\n        mean_f=mean_function,\n        variance_f=variance_function,\n    )\n)\nY_true_easy = mean_function(X_eval)\n\n# Estimators\nnw_model_easy = NadarayaWatsonKernel(x=x, y=y)\nY_nw_easy = nw_model_easy.expectation(X_eval)\n\nkde_model_easy = NadarayaWatsonKernel(\n    x=x,\n    y=y,\n    set_kernel=kernel_setter(kernel_string=\"gaussian\",map_args= {\"bandwidth\": 13}),\n)\nY_kde_easy = kde_model_easy.expectation(X_eval)\n\ncodpy_model_easy = ConditionerKernel(x=x, y=y)\nY_codpy_easy = codpy_model_easy.expectation(X_eval, reg=100)\n\nplt.scatter(x, y, alpha=0.3, s=10, label=\"Samples\")\nplt.plot(X_eval, Y_true_easy, \"k--\", lw=2, label=\"True $E[Y|X]$\")\nplt.plot(X_eval, Y_nw_easy, \"r-\", lw=2, label=\"NW (bw=13)\")\nplt.plot(X_eval, Y_kde_easy, \"b-.\", lw=2, label=\"NW\")\nplt.plot(X_eval, Y_codpy_easy, \"g\", lw=2, label=\"CodPy\")\nplt.title(r\"Conditional Expectation ($\\mu(x) = \\sin(\\pi x)$)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"E[Y|X]\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
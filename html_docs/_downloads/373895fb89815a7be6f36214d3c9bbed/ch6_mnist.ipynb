{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 6.2 Supervised learning: benchmarks of methods with MNIST\n\nHere we reproduce the results of chapter 6.2.2 - Classification problem: handwritten digits.\nWe will compare different models with codpy for a classification task, scoring models on unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Necessary Imports\n ------------------------\n########################################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy\nimport scipy.fft\nimport scipy.linalg\nfrom scipy import ndimage\n\nimport torch\nimport torch.nn as nn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LinearRegression\nimport torchvision.transforms.functional as tf\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets, transforms\nfrom typing import List,Set,Dict,get_type_hints\n\nfrom PIL import Image\n\nimport codpy.core as core\nimport codpy.lalg as lalg\nfrom codpy.data_processing import hot_encoder\nfrom codpy.kernel import KernelClassifier\nfrom codpy.plot_utils import multi_plot\nfrom scipy.special import softmax\nfrom codpydll import *\nfrom codpy.kernel import Kernel,Sampler\nfrom codpy.sampling import get_uniforms,get_normals,get_qmc_uniforms,get_qmc_normals\nfrom codpy.permutation import lsap, map_invertion,Gromov_Monge,dic_invertion\nfrom codpy.conditioning import ConditionerKernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MNIST Data Preparation\nWe normalize pixel values and reshape data for processing.\nPixel data is used as flat vectors, and labels are one-hot encoded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_MNIST_data(N=-1, flatten=True, one_hot=True, seed=43):\n    transform = transforms.ToTensor()\n\n    train_data = datasets.MNIST(\n        root=\".\", train=True, download=True, transform=transform\n    )\n    test_data = datasets.MNIST(\n        root=\".\", train=False, download=True, transform=transform\n    )\n\n    x = train_data.data.numpy().astype(np.float32) / 255.0\n    fx = train_data.targets.numpy()\n\n    z = test_data.data.numpy().astype(np.float32) / 255.0\n    fz = test_data.targets.numpy()\n\n    if flatten:\n        x = x.reshape(len(x), -1)\n        z = z.reshape(len(z), -1)\n\n    if one_hot:\n        fx = np.eye(10)[fx]\n        fz = np.eye(10)[fz]\n    if N==-1:\n        N = x.shape[0]\n    random.seed(seed)\n    indices = random.sample(range(x.shape[0]), min(N,x.shape[0]))\n    x, fx = x[indices], fx[indices]\n        \n    return x, fx, z, fz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification Models\nBelow we define the 3 classification models which will be used in our experiments.\nWe use the accuracy loss as a metric along with Maximum Mean Discrepancy (MMD) for the codpy model to showcase the inverse relationship between the score and MMD.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For classification tasks, use KernelClassifier.\n# This is used the same way as Kernel, but it returns a softmax distribution over classes.\ndef RKHS_ridge_regression(x, fx, z, fz):\n    kernel = KernelClassifier(\n        x=x,\n        fx=fx,\n        clip=None,  # Clipping is used for non-probability inputs\n    )\n    preds = kernel(z)\n    end_time = time.perf_counter()\n    mmd = np.sqrt(kernel.discrepancy(z))\n    return preds, fz, mmd, end_time\n\n\n\nclass conv_classifier(KernelClassifier):\n    def get_kernel(self) -> callable:\n        if not hasattr(self, \"kernel\"):\n            cd.set_kernel(\"tinv_poly_kernel\",{\"p\": str(1)})\n            # ones =\"1;\"*(x.shape[0]-1)+\"1\"\n            # cd.set_kernel(\"tinv_maternnorm\",{\"weights\" : ones})\n            cd.kernel_interface.set_polynomial_order(0)\n            cd.kernel_interface.set_regularization(1e-9)\n            cd.kernel_interface.set_map(\"scale_to_unitcube\")\n            self.kernel = core.KerInterface.get_kernel_ptr()\n        return self.kernel\n        # pass\n\ndef cut(x,N):\n    out = np.array_split(x,N,axis=1)\n    out = [np.concatenate([np.ones([o.shape[0],1])*i,o],axis=1) for i,o in enumerate(out)]\n    return out\n\ndef codpy_tr_model(x, fx, z, fz,N=28):\n    \n    kernels = []\n    xs = []\n    latentxs = []\n    kernel = None\n    x_splitted = cut(x,N)\n    fx_splitted = np.repeat(fx,len(x_splitted),axis=1)\n    x_splitted = [np.concatenate([x_split,fx],axis=1) for x_split,fx_split in zip(x_splitted,fx_splitted)]\n    count = 0\n    values = np.ones_like(fx)\n    values /= values.sum(1)[:,None]\n    for x_split,fx_split in zip(x_splitted,fx_splitted):\n        test=np.linalg.norm(x_split[1:,:N+1])\n        if test > 16:\n            x_split[:,N+1:] = values\n            kernel = KernelClassifier(x=x_split,fx = fx,reg=1e-9)\n            values = kernel(kernel.get_x())\n            kernels += [kernel]\n            count = count + 1\n            xs+= [kernel.get_x()]\n        else:\n            kernels += [None]\n        # latentxs+= [kernel(kernel.get_x())]\n\n    z_splitted = cut(z,N)\n    values = np.ones_like(fz)\n    values /= values.sum(1)[:,None]\n    # values = fz\n    for kernel,z_split in zip(kernels,z_splitted):\n        if kernel is not None:\n            z = np.concatenate([z_split,values],axis=1)\n            values = kernel(z)\n    preds = values\n    mmd=0\n    end_time = time.perf_counter()\n    return preds, fz, mmd, end_time\ndef get_weights2D(x,fx,sigma=.4):\n    out = np.zeros(x.shape[1])\n    for n in range(x.shape[1]):\n       j = n//28 -14\n       i = n%28 - 14\n       out[(i+28*j)%x.shape[1]]+=np.exp(-(i*i+j*j)*sigma)\n    #    out[(i+28*j)%x.shape[1]]+=max(1-np.sqrt(i*i+j*j)*sigma,0.)\n    out /= out.sum()\n    out = scipy.linalg.toeplitz(out)\n    return out\ndef rotate(x,angle=10):\n    images = x.reshape(x.shape[0],28,28)\n    out = x.copy()\n    for n in range(x.shape[0]):\n        image = images[n]\n        # plt.imshow(image,cmap='gray')\n        image = np.asarray(tf.rotate(Image.fromarray(image),angle))\n        # plt.imshow(image,cmap='gray')\n        out[n] = image.ravel()\n    return out\n\ndef get_weights1D(x,fx,sigma=4.):\n    out = np.zeros(x.shape[1])\n    for n in range(x.shape[1]):\n        i=(n-x.shape[1]//2)\n        out[i%x.shape[1]]=np.exp(-i*i*sigma)\n    out /= out.sum()\n    out = scipy.linalg.toeplitz(out)\n    return out\n\n\ndef RKHS_conv_model(x, fx, z, fz,get_weights=get_weights2D):\n    sigma=.5\n    weights = get_weights(x,fx,sigma)\n    xcs = x@weights\n    xrs1=rotate(xcs,-10.)\n    xrs2=rotate(xcs,+10.)\n    xs=np.concatenate([x,xrs1,xrs2])\n    fxs=np.concatenate([fx,fx,fx])   \n    classifier_kernel = KernelClassifier(x=xs,fx=fxs,clip=None)\n    preds = classifier_kernel(z@weights)\n    mmd=0\n    end_time = time.perf_counter()\n    return preds, fz, mmd, end_time\n\n\ndef codpy_fft_model(x, fx, z, fz,get_weights=get_weights2D):\n    xs=scipy.fft.fft(x,workers=-1)\n    # xs = np.real(xs)\n    xs = np.concatenate([x,np.real(xs)],axis=1)\n    classifier_kernel = KernelClassifier(x=xs,fx=fx,clip=None)\n    zs=scipy.fft.fft(z,workers=-1)\n    zs = np.concatenate([z,np.real(zs)],axis=1)\n    # zs = np.real(zs)\n    preds = classifier_kernel(zs)\n    mmd=0\n    end_time = time.perf_counter()\n    return preds, fz, mmd, end_time\n\ndef get_fun(x,fx):\n    left = core.KerOp.dnm(fx,fx,distance=\"norm22\")\n    right = core.KerOp.dnm(x,x,distance=\"norm22\")\n    J1 = (left * right).sum()\n    J2 = (left - right)\n    J2 = (J2*J2).sum()\n    return J1,J2\n\ndef get_J(x,fx,weights=[1,2,4,10,4,2,1]):\n    test = get_fun(x,fx)\n    out = core.KerOp.dnm(fx,fx,distance=\"norm22\")\n    out = out - np.diag(out.sum(1))\n    out = out @ x\n    out = x.T @ out\n    out = np.identity(out.shape[0]) + out / (2.*np.max(np.fabs(out)))\n    return out/out.sum(1)[:,None]\ndef get_A(x,fx,weights=[1,2,4,10,4,2,1]):\n    weights=[np.exp(-n*n*.5) for n in range(-5,5)]\n    out = np.zeros(x.shape[1])\n    for n,w in enumerate(weights):\n       i,j = n-len(weights)//2,0\n       out[(i+28*j)%x.shape[1]]+=w\n       i,j = 0,n-len(weights)//2\n       out[(i+28*j)%x.shape[1]]+=w\n    #    i,j = n-len(weights)//2,n-len(weights)//2\n    #    out[i+28*j]+=w\n    #    i,j = n-len(weights)//2,len(weights)//2 - n\n    #    out[i+28*j]+=w\n    out = scipy.linalg.toeplitz(out)\n    return out/out.sum(1)[:,None]\n\ndef get_A(x,fx):\n    # test = get_fun(x,fx)\n    dfx = core.KerOp.dnm(fx,fx,distance=\"norm22\")/(fx.shape[1]*fx.shape[1])\n    dx = core.KerOp.dnm(x,x,distance=\"norm22\")/(x.shape[1]*x.shape[1])\n    D = dfx-dx\n    B = -np.ones(x.shape[0])\n    B = scipy.linalg.toeplitz(B)\n    B *= D\n    B -= np.diag(B.sum(1))\n\n    xs = B@x\n    out = x.T @ xs\n    U,D = lalg.LAlg.self_adjoint_eigen_decomposition(out)\n    D =np.array(D)\n    D = np.exp(-D)\n    out = U@np.diag(D)@U.T\n    test = get_fun(x@out,fx)\n\n    return out\n\n\n   \ndef codpy_convolutional_model(x, fx, z, fz):\n    kernel = conv_classifier(\n        x=x,\n        fx=fx,\n        clip=None,  # Clipping is used for non-probability inputs\n    )\n    preds = kernel(z)\n    end_time = time.perf_counter()\n\n    # mmd = np.sqrt(kernel.discrepancy(z))\n    mmd=0\n\n    return preds, fz, mmd, end_time\n\ndef random_forest_model(x, fx, z, fz):\n    # Convert inputs to numpy arrays and one-hot to labels\n    x = np.array(x)\n    fx_labels = np.argmax(np.array(fx), axis=1)\n    z = np.array(z)\n    fz_labels = np.argmax(np.array(fz), axis=1)\n\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(x, fx_labels)\n    results = model.predict(z)\n\n    num_classes = 10  # MNIST has 10 classes (0-9)\n    results_oh = np.zeros((len(results), num_classes))\n    results_oh[np.arange(len(results)), results] = 1\n\n    # Also convert true labels to one-hot for consistent return\n    fz_oh = np.zeros((len(fz_labels), num_classes))\n    fz_oh[np.arange(len(fz_labels)), fz_labels] = 1\n\n    return results_oh, fz_oh, None, time.perf_counter()\n\n\ndef torch_model(x, fx, z, fz):\n    x = torch.tensor(x, dtype=torch.float32)\n    fx = torch.tensor(fx, dtype=torch.long).argmax(dim=1)\n    z = torch.tensor(z, dtype=torch.float32)\n    fz = torch.tensor(fz, dtype=torch.long)\n    out_shape = fz.shape[1]\n\n    class FFN(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(256, 128),\n                nn.BatchNorm1d(128),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(128, 64),\n                nn.BatchNorm1d(64),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(64, output_dim),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    model = FFN(x.shape[1], out_shape)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    loss_fn = nn.CrossEntropyLoss()\n\n    n_samples = x.shape[0]\n    batch_size = max(n_samples // 4, 32)\n\n    model.train()\n    for epoch in range(30):\n        indices = torch.randperm(n_samples)\n        for start in range(0, n_samples, batch_size):\n            end = min(start + batch_size, n_samples)\n            batch_idx = indices[start:end]\n\n            x_batch = x[batch_idx]\n            fx_batch = fx[batch_idx]\n\n            pred = model(x_batch)\n            loss = loss_fn(pred, fx_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        results = model(z)\n\n    return results.cpu(), fz.cpu(), None, time.perf_counter()\n\n\ndef Perceptron(x, fx, z, fz):\n    x = torch.tensor(x, dtype=torch.float32)\n    fx = torch.tensor(fx, dtype=torch.float32).argmax(dim=1)\n    z = torch.tensor(z, dtype=torch.float32)\n    fz = torch.tensor(fz, dtype=torch.float32)\n    out_shape = fz.shape[1]\n\n    class FFN(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 8),\n                nn.ReLU(),\n                nn.Linear(8, 32),\n                nn.ReLU(),\n                nn.Linear(32, 64),\n                nn.ReLU(),\n                nn.Linear(64, output_dim),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    model = FFN(x.shape[1], out_shape)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    loss_fn = nn.CrossEntropyLoss()\n\n    n_samples = x.shape[0]\n    batch_size = n_samples // 4\n    for epoch in range(128):\n        # Shuffle data each epoch\n        indices = torch.randperm(n_samples)\n\n        for start in range(0, n_samples, batch_size):\n            end = min(start + batch_size, n_samples)\n            batch_idx = indices[start:end]\n\n            x_batch = x[batch_idx]\n            fx_batch = fx[batch_idx]\n\n            pred = model(x_batch)\n            loss = loss_fn(pred, fx_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    with torch.no_grad():\n        results = model(z)\n    return results, fz, None, time.perf_counter()\n\n\ndef vgg_torch_model(x, fx, z, fz):\n    x = torch.tensor(x, dtype=torch.float32).view(-1, 1, 28, 28)\n    fx = torch.tensor(fx, dtype=torch.float32).argmax(dim=1).long()\n\n    z = torch.tensor(z, dtype=torch.float32).view(-1, 1, 28, 28)\n    fz = torch.tensor(fz, dtype=torch.float32).long()  # .argmax(dim=1).long()\n    out_shape = fz.shape[1]\n\n    dataset = TensorDataset(x, fx)\n    trainloader = DataLoader(dataset, batch_size=min(128, x.shape[0]), shuffle=True)\n\n    class VGG(nn.Module):\n        def __init__(self, input_dim=1, output_dim=10):\n            super().__init__()\n            self.features = nn.Sequential(\n                nn.Conv2d(\n                    in_channels=input_dim, out_channels=32, kernel_size=3, padding=1\n                ),\n                nn.BatchNorm2d(32),\n                nn.ReLU(),\n                nn.Dropout2d(0.2),\n                nn.MaxPool2d(2),\n                nn.Conv2d(32, 64, 3, padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n                nn.Dropout2d(0.3),\n                nn.MaxPool2d(2),\n            )\n\n            self.classifier = nn.Sequential(\n                nn.Flatten(),\n                nn.Linear(64 * 7 * 7, 128),\n                nn.ReLU(),\n                nn.Dropout(0.5),\n                nn.Linear(128, output_dim),\n            )\n\n        def forward(self, x):\n            return self.classifier(self.features(x))\n\n    model = VGG(x.shape[1], out_shape)\n    opt = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(30):\n        for x_batch, fx_batch in trainloader:\n            pred = model(x_batch)\n            loss = loss_fn(pred, fx_batch)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n    with torch.no_grad():\n        results = model(z)\n    return results, fz, None, time.perf_counter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gathering Results\nWe train the models on different subsets of the training dataset.\nWe always evaluate the models on the entire test set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "core.KerInterface.set_verbose()\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = {}\ntimes = {}\nmmds = {}\nmodels = [\n    RKHS_conv_model,\n    RKHS_ridge_regression,\n    # torch_model,\n    Perceptron,\n    # random_forest_model,\n    vgg_torch_model,\n]\nmodel_aliases = {\n    \"RKHS_conv_model\": \"K_CM\",\n    \"RKHS_ridge_regression\": \"K_RR\",\n    # \"torch_model\": \"FFN\",\n    \"Perceptron\": \"NN_PM\",\n    # \"random_forest_model\": \"RF\",\n    \"vgg_torch_model\": \"NN_VGG\",\n}\n\nX, FX, Z, FZ = get_MNIST_data()\n# torch.manual_seed(0)\n# idxs = torch.randperm(len(X))\n# idxs_z = torch.randperm(len(Z))\nSIZE = 2**11\n# X, FX, Z, FZ = X[idxs][:SIZE], FX[idxs][:SIZE], Z[idxs_z][:SIZE], FZ[idxs_z][:SIZE]\nlength_ = len(X)\nscenarios_list = [\n    (int(i), int(i), -1, -1) for i in 2 ** np.arange(8, 12)\n]\n\nfor scenario in scenarios_list:\n    Nx, Nfx, Nz, Nfz = scenario\n    x, fx, z, fz = X[:Nx], FX[:Nfx], Z[:Nz], FZ[:Nfz]\n    # x, fx, z, fz = X[:Nx], FX[:Nfx], X[:Nx], FX[:Nfx]\n    results[len(x)] = {}\n    times[len(x)] = {}\n    # mmds[len(x)] = {}\n    for model in models:\n        start_time = time.perf_counter()\n        logits, target, mmd, end_time = model(x, fx, z, fz)\n        # mmds[len(x)][model.__name__] = mmd\n        pred_classes = np.argmax(logits, axis=1)\n        true_classes = np.argmax(target, axis=1)\n        accuracy = accuracy_score(true_classes, pred_classes)\n        results[len(x)][model.__name__] = accuracy\n        times[len(x)][model.__name__] = end_time - start_time\n        print(\n            f\"Model: {model.__name__}, size: {Nx}, Time taken: {times[len(x)][model.__name__]:.4f} seconds, Accuracy: {results[len(x)][model.__name__]:.4f} %\"\n        )\nres = [{\"data\": results}, {\"data\": times}]\n# res = [{\"data\": results}, {\"data\": mmds}, {\"data\": times}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting\n ------------------------\n########################################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_one(inputs):\n    results = inputs[\"data\"]\n    ax = inputs[\"ax\"]\n    legend = inputs[\"legend\"]\n\n    x_vals = sorted(results.keys())\n    for model_name in next(iter(results.values())).keys():\n        y_vals = [results[x][model_name] for x in x_vals]\n        label = model_aliases.get(model_name, model_name)\n        ax.plot(x_vals, y_vals, marker=\"o\", label=label)\n\n    ax.set_xlabel(\"Number of Training Examples\")\n    ax.set_ylabel(legend)\n    ax.legend()\n    ax.grid(True)\n\n    return ax\n\n\nmulti_plot(\n    res,\n    plot_one,\n    mp_nrows=1,\n    mp_ncols=3,\n    legends=[\"Accuracy\", \"Times\"],\n    mp_figsize=(14, 10),\n)\nplt.show()\npass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
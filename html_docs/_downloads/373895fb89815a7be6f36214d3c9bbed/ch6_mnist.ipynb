{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 6.2 Supervised learning: benchmarks of methods with MNIST\n\nHere we reproduce the results of chapter 6.2.2 - Classification problem: handwritten digits.\nWe will compare different models with codpy for a classification task, scoring models on unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Necessary Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom codpydll import *\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets, transforms\n\nimport codpy.core as core\nfrom codpy.kernel import KernelClassifier\nfrom codpy.plot_utils import multi_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MNIST Data Preparation\nWe normalize pixel values and reshape data for processing.\nPixel data is used as flat vectors, and labels are one-hot encoded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_MNIST_data(N=-1, flatten=True, one_hot=True, seed=43):\n    transform = transforms.ToTensor()\n\n    train_data = datasets.MNIST(\n        root=\".\", train=True, download=True, transform=transform\n    )\n    test_data = datasets.MNIST(\n        root=\".\", train=False, download=True, transform=transform\n    )\n\n    x = train_data.data.numpy().astype(np.float32) / 255.0\n    fx = train_data.targets.numpy()\n\n    z = test_data.data.numpy().astype(np.float32) / 255.0\n    fz = test_data.targets.numpy()\n\n    if flatten:\n        x = x.reshape(len(x), -1)\n        z = z.reshape(len(z), -1)\n\n    if one_hot:\n        fx = np.eye(10)[fx]\n        fz = np.eye(10)[fz]\n    if N == -1:\n        N = x.shape[0]\n    indices = random.sample(range(x.shape[0]), min(N, x.shape[0]))\n    x, fx = x[indices], fx[indices]\n\n    return x, fx, z, fz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification Models\nBelow we define the 3 classification models which will be used in our experiments.\nWe use the accuracy loss as a metric along with Maximum Mean Discrepancy (MMD) for the codpy model to showcase the inverse relationship between the score and MMD.\nFor classification tasks, use KernelClassifier.\nThis is used the same way as Kernel, but it returns a softmax distribution over classes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def codpy_model(x, fx, z, fz):\n    kernel = KernelClassifier(\n        x=x,\n        fx=fx,\n        clip=None,  # Clipping is used for non-probability inputs\n    )\n    preds = kernel(z)\n    end_time = time.perf_counter()\n    # mmd = np.sqrt(kernel.discrepancy(z))\n    mmd = 0\n\n    return preds, fz, mmd, end_time\n\n\ndef codpy_convolutional_model(x, fx, z, fz):\n    class myclassifier(KernelClassifier):\n        def get_kernel(self) -> callable:\n            if not hasattr(self, \"kernel\"):\n                cd.set_kernel(\"tinv_poly_kernel\", {\"p\": str(1)})\n                # ones = \"1;\" * (x.shape[0] - 1) + \"1\"\n                # cd.set_kernel(\"tinv_maternnorm\")\n                cd.kernel_interface.set_polynomial_order(0)\n                cd.kernel_interface.set_regularization(1e-9)\n                cd.kernel_interface.set_map(\"scale_to_unitcube\")\n                self.kernel = core.KerInterface.get_kernel_ptr()\n            return self.kernel\n\n    kernel = myclassifier(\n        x=x,\n        fx=fx,\n        clip=None,  # Clipping is used for non-probability inputs\n    )\n    preds = kernel(z)\n    end_time = time.perf_counter()\n\n    # mmd = np.sqrt(kernel.discrepancy(z))\n    mmd = 0\n\n    return preds, fz, mmd, end_time\n\n\ndef random_forest_model(x, fx, z, fz):\n    # Convert inputs to numpy arrays and one-hot to labels\n    x = np.array(x)\n    fx_labels = np.argmax(np.array(fx), axis=1)\n    z = np.array(z)\n    fz_labels = np.argmax(np.array(fz), axis=1)\n\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(x, fx_labels)\n    results = model.predict(z)\n\n    num_classes = 10  # MNIST has 10 classes (0-9)\n    results_oh = np.zeros((len(results), num_classes))\n    results_oh[np.arange(len(results)), results] = 1\n\n    # Also convert true labels to one-hot for consistent return\n    fz_oh = np.zeros((len(fz_labels), num_classes))\n    fz_oh[np.arange(len(fz_labels)), fz_labels] = 1\n\n    return results_oh, fz_oh, None, time.perf_counter()\n\n\ndef torch_model(x, fx, z, fz):\n    x = torch.tensor(x, dtype=torch.float32)\n    fx = torch.tensor(fx, dtype=torch.long).argmax(dim=1)\n    z = torch.tensor(z, dtype=torch.float32)\n    fz = torch.tensor(fz, dtype=torch.long)\n    out_shape = fz.shape[1]\n    # batch_size = max(x.shape[0] // 4, 32)\n\n    dataset = TensorDataset(x, fx)\n    trainloader = DataLoader(dataset, batch_size=min(32, x.shape[0]), shuffle=True)\n\n    class FFN(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(256, 128),\n                nn.BatchNorm1d(128),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(128, 64),\n                nn.BatchNorm1d(64),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(64, output_dim),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    model = FFN(x.shape[1], out_shape)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    loss_fn = nn.CrossEntropyLoss()\n\n    model.train()\n    for epoch in range(50):\n        for x_batch, fx_batch in trainloader:\n            pred = model(x_batch)\n            loss = loss_fn(pred, fx_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        results = model(z)\n\n    return results.cpu(), fz.cpu(), None, time.perf_counter()\n\n\ndef torch_model_naive(x, fx, z, fz):\n    x = torch.tensor(x, dtype=torch.float32)\n    fx = torch.tensor(fx, dtype=torch.float32).argmax(dim=1)\n    z = torch.tensor(z, dtype=torch.float32)\n    fz = torch.tensor(fz, dtype=torch.float32)\n    out_shape = fz.shape[1]\n\n    # batch_size = x.shape[0] // 4\n    dataset = TensorDataset(x, fx)\n    trainloader = DataLoader(dataset, batch_size=min(32, x.shape[0]), shuffle=True)\n\n    class FFN(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 8),\n                nn.ReLU(),\n                nn.Linear(8, 32),\n                nn.ReLU(),\n                nn.Linear(32, 64),\n                nn.ReLU(),\n                nn.Linear(64, output_dim),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    model = FFN(x.shape[1], out_shape)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(128):\n        for x_batch, fx_batch in trainloader:\n            pred = model(x_batch)\n            loss = loss_fn(pred, fx_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    with torch.no_grad():\n        results = model(z)\n    return results, fz, None, time.perf_counter()\n\n\ndef vgg_torch_model(x, fx, z, fz):\n    x = torch.tensor(x, dtype=torch.float32).view(-1, 1, 28, 28)\n    fx = torch.tensor(fx, dtype=torch.float32).argmax(dim=1).long()\n\n    z = torch.tensor(z, dtype=torch.float32).view(-1, 1, 28, 28)\n    fz = torch.tensor(fz, dtype=torch.float32).long()  # .argmax(dim=1).long()\n    out_shape = fz.shape[1]\n\n    dataset = TensorDataset(x, fx)\n    trainloader = DataLoader(dataset, batch_size=min(128, x.shape[0]), shuffle=True)\n\n    class VGG(nn.Module):\n        def __init__(self, input_dim=1, output_dim=10):\n            super().__init__()\n            self.features = nn.Sequential(\n                nn.Conv2d(\n                    in_channels=input_dim, out_channels=32, kernel_size=3, padding=1\n                ),\n                nn.BatchNorm2d(32),\n                nn.ReLU(),\n                nn.Dropout2d(0.2),\n                nn.MaxPool2d(2),\n                nn.Conv2d(32, 64, 3, padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n                nn.Dropout2d(0.3),\n                nn.MaxPool2d(2),\n            )\n\n            self.classifier = nn.Sequential(\n                nn.Flatten(),\n                nn.Linear(64 * 7 * 7, 128),\n                nn.ReLU(),\n                nn.Dropout(0.5),\n                nn.Linear(128, output_dim),\n            )\n\n        def forward(self, x):\n            return self.classifier(self.features(x))\n\n    model = VGG(x.shape[1], out_shape)\n    opt = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(100):\n        for x_batch, fx_batch in trainloader:\n            pred = model(x_batch)\n            loss = loss_fn(pred, fx_batch)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n    with torch.no_grad():\n        results = model(z)\n    return results, fz, None, time.perf_counter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gathering Results\nWe train the models on different subsets of the training dataset.\nWe always evaluate the models on the entire test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = {}\ntimes = {}\nmmds = {}\nmodels = [\n    torch_model,\n    torch_model_naive,\n    random_forest_model,\n    codpy_convolutional_model,\n    codpy_model,\n    vgg_torch_model,\n]\n\nX, FX, Z, FZ = get_MNIST_data()\n# torch.manual_seed(0)\n# idxs = torch.randperm(len(X))\n# idxs_z = torch.randperm(len(Z))\nSIZE = 2**11\n# X, FX, Z, FZ = X[idxs][:SIZE], FX[idxs][:SIZE], Z[idxs_z][:SIZE], FZ[idxs_z][:SIZE]\nlength_ = len(X)\nscenarios_list = [\n    (int(i), int(i), -1, -1) for i in 2 ** np.arange(np.log2(16), np.log2(SIZE) + 1)\n]\n\nfor scenario in scenarios_list:\n    Nx, Nfx, Nz, Nfz = scenario\n    x, fx, z, fz = X[:Nx], FX[:Nfx], Z[:Nz], FZ[:Nfz]\n    results[len(x)] = {}\n    times[len(x)] = {}\n    mmds[len(x)] = {}\n    for model in models:\n        start_time = time.perf_counter()\n        logits, target, mmd, end_time = model(x, fx, z, fz)\n        mmds[len(x)][model.__name__] = mmd\n        pred_classes = np.argmax(logits, axis=1)\n        true_classes = np.argmax(target, axis=1)\n        accuracy = accuracy_score(true_classes, pred_classes)\n        results[len(x)][model.__name__] = accuracy\n        times[len(x)][model.__name__] = end_time - start_time\n        print(\n            f\"Model: {model.__name__}, size: {Nx}, Time taken: {times[len(x)][model.__name__]:.4f} seconds, Accuracy: {results[len(x)][model.__name__]:.4f} seconds\"\n        )\n\nres = [{\"data\": results}, {\"data\": times}]\n# res = [{\"data\": results}, {\"data\": mmds}, {\"data\": times}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting\n ------------------------\n########################################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_one(inputs):\n    results = inputs[\"data\"]\n    ax = inputs[\"ax\"]\n    legend = inputs[\"legend\"]\n\n    model_aliases = {\n        \"torch_model\": \"FFN\",\n        \"torch_model_naive\": \"FFN: basic\",\n        \"codpy_model\": \"KRR\",\n        \"codpy_convolutional_model\": \"CKRR\",\n        \"random_forest_model\": \"RF\",\n        \"vgg_torch_model\": \"VGG\",\n    }\n\n    model_markers = {\n        \"torch_model\": \"o\",\n        \"torch_model_naive\": \"s\",\n        \"codpy_model\": \"D\",\n        \"codpy_convolutional_model\": \"^\",\n        \"random_forest_model\": \"v\",\n        \"vgg_torch_model\": \"X\",\n    }\n\n    x_vals = sorted(results.keys())\n    for model_name in next(iter(results.values())).keys():\n        y_vals = [results[x][model_name] for x in x_vals]\n        label = model_aliases.get(model_name, model_name)\n        marker = model_markers.get(model_name, \"o\")\n        ax.plot(x_vals, y_vals, marker=marker, label=label)\n\n    ax.set_xlabel(\"Number of Training Examples\", fontsize=14)\n    ax.set_ylabel(legend, fontsize=14)\n    ax.tick_params(axis=\"both\", labelsize=12)\n    ax.legend(fontsize=12)\n    ax.grid(True)\n\n    return ax\n\n\nmulti_plot(\n    res,\n    plot_one,\n    mp_nrows=1,\n    mp_ncols=2,  # 3\n    legends=[\"Accuracy\", \"Times\"],\n    # legends=[\"Accuracy\", \"MMD\", \"Times\"],\n    mp_figsize=(14, 10),\n)\nplt.show()\npass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
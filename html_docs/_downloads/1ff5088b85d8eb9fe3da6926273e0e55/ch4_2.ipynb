{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 4.2 Clustering\nIn this section, we will explore clustering methods using the CodPy library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import necessary modules and setup the environment\nimport os\nimport random\nimport sys\nimport time\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import pairwise_distances\n\nimport codpy.core as core\nfrom codpy.clustering import RandomClusters, MiniBatchkmeans, BalancedClustering, GreedySearch, SharpDiscrepancy\nfrom codpy.kernel import Kernel\nfrom codpy.plot_utils import multi_plot\n\nwarnings.filterwarnings(\"ignore\")\n\ntry:\n    current_dir = os.path.dirname(__file__)\n    data_dir = os.path.join(current_dir, \"data\")\nexcept NameError:\n    current_dir = os.getcwd()\n    data_dir = os.path.join(current_dir, \"data\")\n\n# Ensure utils path is in sys.path\ncurr_f = os.path.join(os.getcwd(), \"codpy-book\", \"utils\")\nsys.path.insert(0, curr_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Define Functions for Generating Blob Data and Clustering**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gen_blobs(Nx, Ny, D):\n    \"\"\"\n    Generate blob data for clustering.\n\n    Parameters:\n    - Nx: Number of samples for training.\n    - Nz: Number of samples for testing.\n    - Ny: Number of centers (clusters).\n    - D: Number of dimensions (features).\n\n    Returns:\n    - X: Generated data points.\n    - y: Cluster labels for each point.\n    \"\"\"\n    X, y = make_blobs(\n        n_samples=Nx,\n        n_features=int(D),\n        centers=int(Ny),\n        cluster_std=1,\n        center_box=(-10.0, 10.0),\n        shuffle=True,\n        random_state=1,\n    )\n    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clustering Models\n This section defines the K-means and CodPy clustering models.\n\n########################################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def greedy_clustering(x, Ny):\n    \"\"\"\n    Apply a fast greedy search algorithms for clustering.\n\n    Parameters:\n    - x: Training data.\n    - Ny: Number of clusters.\n\n    Returns:\n    - fx: Predicted cluster labels for test data.\n    - centers: Selected cluster centers.\n    \"\"\"\n    # Set up a kernel and select centers\n    kernel = GreedySearch(x=x, N=Ny, set_kernel=set_gaussian_kernel, all=True)\n    # retrieve the centers\n    centers = kernel.cluster_centers_\n    # retrieve labels associated to a set of points\n    fx = kernel.get_labels()\n    return fx, centers\n\n\ndef sharp_clustering(x, Ny):\n    # Set up a kernel and select centers\n    kernel = SharpDiscrepancy(x=x, N=Ny, set_kernel=set_gaussian_kernel)\n    centers = kernel.cluster_centers_\n    labels = kernel.get_labels()\n    return labels, centers\n\ndef balanced_sharp_clustering(x, Ny):\n    # Set up a kernel and select centers\n    kernel = BalancedClustering(SharpDiscrepancy, x=x, N=Ny)\n    centers = kernel.cluster_centers_\n    labels = kernel.get_labels()\n    return labels, centers\n\ndef kmeans_clustering(x, Ny):\n    kernel = KMeans(n_clusters=Ny, random_state=1).fit(x)\n    centers = kernel.cluster_centers_\n    labels = kernel.labels_\n    return labels, centers\n\ndef balanced_kmeans_clustering(x, Ny):\n    kernel = BalancedClustering(MiniBatchkmeans, x=x, N=Ny)\n    centers = kernel.cluster_centers_\n    labels = kernel.labels_\n    return labels, centers\n\ndef balanced_greedy_clustering(x, Ny):\n    method = BalancedClustering(GreedySearch, x=x, N=Ny)\n    centers = method.cluster_centers_\n    labels = method.get_labels()\n    return labels, centers\n\n\n# class Random_clusters:\n#     def __init__(self, x, N, **kwargs):\n#         self.x = x\n#         self.indices = random.sample(range(self.x.shape[0]), N)\n#         self.cluster_centers_ = self.x[self.indices]\n\n#     def __call__(self, z, N=None, **kwargs):\n#         return self.distance(z, self.cluster_centers_).argmin(axis=1)\n\n#     def distance(self, x, y):\n#         Kernel(x=x, set_kernel=set_gaussian_kernel)\n#         return core.KerOp.dnm(x, y)\n\n\ndef balanced_random_clustering(x, Ny):\n    method = BalancedClustering(RandomClusters, x=x, N=Ny)\n    centers = method.cluster_centers_\n    labels = method.get_labels()\n    return labels, centers\n\ndef random_clustering(x, Ny):\n    method = RandomClusters(x=x, N=Ny)\n    centers = method.cluster_centers_\n    labels = method.get_labels()\n    return labels, centers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the Experiment\n This section runs the experiment to compare K-means and various CodPy clustering.\n\n########################################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "set_gaussian_kernel = core.kernel_setter(kernel_string=\"gaussian\", map_string=\"standardmean\")\n\n\ndef compute_mmd(x_test, z):\n    kernel = Kernel(x=x_test, order=0, set_kernel=set_gaussian_kernel)\n    mmd = kernel.discrepancy(z)\n    return mmd\n\n\ndef compute_inertia(x, y):\n    return np.sum((pairwise_distances(x, y) ** 2).min(axis=1))\n\n\ndef one_experiment(X, Ny, get_predictor):\n    def get_score(X, cluster_centers):\n        inertia = compute_inertia(X, cluster_centers)\n        # Calculate MMD for K-means\n        mmd = compute_mmd(X, cluster_centers)\n        return inertia, mmd\n\n    elapsed_time = time.time()\n    labels, cluster_centers = get_predictor(X, Ny)\n    elapsed_time = time.time() - elapsed_time\n    inertia, mmd = get_score(X, cluster_centers)\n    return inertia, mmd, elapsed_time\n\n\ndef run_experiment(Nx_values, D, Ny_values, get_predictors, labels, file_name=None):\n    results = []\n    for Nx in Nx_values:\n        for Ny in Ny_values:\n            X, _ = gen_blobs(Nx, Ny, D)\n            for get_predictor, label in zip(get_predictors, labels):\n                inertia, mmd, elapsed_time = one_experiment(X, Ny, get_predictor)\n                print(\n                    \"Method:\",\n                    label,\n                    \"N_partition:\",\n                    Ny,\n                    \" inertia:\",\n                    inertia,\n                    \" mmd:\",\n                    mmd,\n                    \" time:\",\n                    elapsed_time,\n                )\n                results.append(\n                    {\n                        \"Method\": label,\n                        \"Nx\": Nx,\n                        \"Ny\": Ny,\n                        \"Execution Time (s)\": elapsed_time,\n                        \"inertia\": inertia,\n                        \"mmd\": mmd,\n                    }\n                )\n    out = pd.DataFrame(results)\n    print(out)\n    if file_name is not None:\n        out.to_csv(file_name, index=False)\n    return out\n\n\ndef plot_experiment(Nx_values, D, Ny_values, get_predictors, names):\n    \"\"\"\n    Run the clustering experiment with different numbers of clusters (Ny) and visualize the results.\n\n    Parameters:\n    - Nx: Number of samples for training.\n    - Nz: Number of samples for testing\n    - D: Number of dimensions (features).\n    - Ny_values: List of cluster counts to experiment with.\n    \"\"\"\n    results, legends = [], []\n\n    def one_experiment(X, Ny, get_predictor):\n        labels, cluster_centers = get_predictor(X, Ny)\n        return labels, cluster_centers\n\n    for Nx in Nx_values:\n        for Ny in Ny_values:\n            X, _ = gen_blobs(Nx, Ny, D)\n            for get_predictor, name in zip(get_predictors, names):\n                labels, cluster_centers = one_experiment(X, Ny, get_predictor)\n                print(\"Method:\", name, \"N_partition:\", Ny)\n                results.append((X, labels, cluster_centers))\n                legends.append(name + f\",  {Ny}\" + \" clusters\")\n\n    # Plot the clustering results using multi_plot\n    def plot_clusters(data, ax, legend=\"\", **kwargs):\n        x, labels, centers = data\n        ax.scatter(x[:, 0], x[:, 1], c=labels, cmap=\"viridis\", alpha=0.5)\n        if centers is not None:\n            ax.scatter(\n                centers[:, 0],\n                centers[:, 1],\n                c=\"red\",\n                marker=\"X\",\n                s=200,\n                label=\"Centers\",\n            )\n        ax.title.set_text(legend)\n    multi_plot(\n        results,\n        plot_clusters,\n        mp_nrows=2,\n        mp_ncols=len(get_predictors)//2,\n        mp_figsize=(14, 10),\n        legends=legends,\n    )\n\n\nif __name__ == \"__main__\":\n    get_predictors = [\n        greedy_clustering,\n        sharp_clustering,\n        kmeans_clustering,\n        random_clustering,\n        balanced_greedy_clustering,\n        balanced_sharp_clustering,\n        balanced_kmeans_clustering,\n        balanced_random_clustering,\n    ]\n    labels = [model.__name__.removesuffix(\"_clustering\") for model in get_predictors]\n    file_name = [\"clustering.csv\"]\n    # Run the experiment\n    Nxs, D, Nys = [1024], 2, [5]\n    # core.KerInterface.set_verbose()\n\n    file_name = os.path.join(data_dir, \"clustering.csv\")\n    run_experiment(Nxs, D, [128], get_predictors, labels, file_name=file_name)\n    plot_experiment(Nxs, D, Nys, get_predictors, labels)\n    plt.show()\n    pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
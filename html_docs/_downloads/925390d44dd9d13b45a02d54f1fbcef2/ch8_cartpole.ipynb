{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 8.2 Experiments - Cartpole\nWe use the OpenAI Gym library to instanciate the gymnasium CartPole-v1 environment and reproduce the figure from chapter 8_XXX. \n\nWe train the following agents: \n\n- PPO \n- DQN \n- Controller-based\n- Kernel Actor-Critic\n- Kernel Q-Learning\n- Kernel Q-Learning HJB\n- Kernel Policy-Gradient\n\nWe show how you can tweak some methods in each algorithm to tune them to the environment. For a detailed documentation on KAgents, see **codpy documentation**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Importing necessary modules\nimport sys\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport codpy.core as core\nimport codpy.KQLearning as KQLearning\n\nfrom ignore_utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KQLearning\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class KQLearningCP(KQLearning.KQLearning):\n\n    def format(self, sarsd, max_training_game_size=None, **kwargs):\n        \"\"\"\n        In Cartpole, we only want to keep a certain amount of timesteps for each episode. The original format approach keep all the data.\n        \"\"\"\n        states, actions, next_states, rewards, dones = [\n            core.get_matrix(e) for e in sarsd\n        ]\n\n        actions = KQLearning.rl_hot_encoder(actions, self.actions_dim)\n        returns = self.compute_returns(\n            states, actions, next_states, rewards, dones, **kwargs\n        )\n        dones = core.get_matrix(dones, dtype=bool)\n        if max_training_game_size is not None:\n            states, actions, next_states, rewards, returns, dones = (\n                states[:max_training_game_size],\n                actions[:max_training_game_size],\n                next_states[:max_training_game_size],\n                rewards[:max_training_game_size],\n                returns[:max_training_game_size],\n                dones[:max_training_game_size],\n            )\n\n        return states, actions, next_states, rewards, returns, dones\n\n    def train(self, game, max_training_game_size =sys.maxsize,tol=1e-4,**kwargs):\n        \"\"\"\n        In cartpole we don't want clustering so we override the train method. \n        \"\"\"\n        states, actions, next_states, rewards, dones = game\n\n        # In cartpole we skip training if we already solved the environment.\n        if len(states) >= kwargs.get(\"max_game\", 1e12):\n            print(\"no training\")\n            return\n        states, actions, next_states, rewards, returns, dones = self.format(game, max_training_game_size=max_training_game_size,**kwargs)\n        if self.critic.is_valid():\n            returns = self.critic(np.concatenate([states,actions],axis=1))\n\n        self.replay_buffer.push(states, actions, next_states, rewards, returns, dones)\n        games = self.replay_buffer.memory\n\n        # self.critic here is a kernel, and it fit on the entire replay buffer to solve for Bellman equations.\n        self.critic = self.optimal_states_values_function(games,verbose=True,**kwargs)        \n        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PolicyGradient\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PolicyGradientCP(KQLearning.PolicyGradient):\n\n    def format(self, sarsd, max_training_game_size=None, **kwargs):\n        states, actions, next_states, rewards, dones = [\n            core.get_matrix(e) for e in sarsd\n        ]\n\n        actions = KQLearning.rl_hot_encoder(actions, self.actions_dim)\n        returns = self.compute_returns(\n            states, actions, next_states, rewards, dones, **kwargs\n        )\n        dones = core.get_matrix(dones, dtype=bool)\n        if max_training_game_size is not None:\n            states, actions, next_states, rewards, returns, dones = (\n                states[:max_training_game_size],\n                actions[:max_training_game_size],\n                next_states[:max_training_game_size],\n                rewards[:max_training_game_size],\n                returns[:max_training_game_size],\n                dones[:max_training_game_size],\n            )\n\n        return states, actions, next_states, rewards, returns, dones\n\n\n    def train(self, game, **kwargs):\n        states, actions, next_states, rewards, dones = game\n        if len(states) >= kwargs.get(\"max_game\", 1e12):\n            print(\"no training\")\n            return\n        super().train(game,clip=1., **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KActorCritic\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class KActorCriticCP(KQLearning.KActorCritic):\n\n    def format(self, sarsd, max_training_game_size=None, **kwargs):\n        \"\"\"\n        Format the game data by keeping only up to max_trainin_game_size timesteps. \n\n        Parameters:\n        - sarsd: tuple collection of game data (states, actions, next_states, rewards, dones).\n        - max_training_game_size: maximum number of timesteps to keep for training.\n\n        Returns:\n        - states, actions, next_states, rewards, returns, dones: formatted game data.\n        \"\"\"\n        states, actions, next_states, rewards, dones = [\n            core.get_matrix(e) for e in sarsd\n        ]\n\n        actions = KQLearning.rl_hot_encoder(actions, self.actions_dim)\n        returns = self.compute_returns(\n            states, actions, next_states, rewards, dones, **kwargs\n        )\n        dones = core.get_matrix(dones, dtype=bool)\n        if max_training_game_size is not None:\n            states, actions, next_states, rewards, returns, dones = (\n                states[:max_training_game_size],\n                actions[:max_training_game_size],\n                next_states[:max_training_game_size],\n                rewards[:max_training_game_size],\n                returns[:max_training_game_size],\n                dones[:max_training_game_size],\n            )\n\n        return states, actions, next_states, rewards, returns, dones\n\n    def train(self, game, **kwargs):\n        \"\"\"\n        Skips training if the game was too long. (for cartpole, this means we already solved the environment.)\n        \"\"\"\n        states, actions, next_states, rewards, dones = game\n        if len(states) >= kwargs.get(\"max_game\", 1e12):\n            print(\"no training\")\n            return\n        super().train(game, clip=1.,**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HJB\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class KQLearningHJBCP(KQLearning.KQLearningHJB):\n\n    def format(self, sarsd, max_training_game_size=None, **kwargs):\n        states, actions, next_states, rewards, dones = [\n            core.get_matrix(e) for e in sarsd\n        ]\n\n        actions = KQLearning.rl_hot_encoder(actions, self.actions_dim)\n        returns = self.compute_returns(\n            states, actions, next_states, rewards, dones, **kwargs\n        )\n        dones = core.get_matrix(dones, dtype=bool)\n        if max_training_game_size is not None:\n            states, actions, next_states, rewards, returns, dones = (\n                states[:max_training_game_size],\n                actions[:max_training_game_size],\n                next_states[:max_training_game_size],\n                rewards[:max_training_game_size],\n                returns[:max_training_game_size],\n                dones[:max_training_game_size],\n            )\n\n        return states, actions, next_states, rewards, returns, dones\n\n\n    def train(self, game, max_training_game_size =sys.maxsize,tol=1e-4,**kwargs):\n        states, actions, next_states, rewards, dones = game\n\n        if len(states) >= kwargs.get(\"max_game\", 1e12):\n            print(\"no training\")\n            return\n        states, actions, next_states, rewards, returns, dones = self.format(game, max_training_game_size=max_training_game_size,**kwargs)\n\n        self.replay_buffer.push(states, actions, next_states, rewards, returns, dones)\n        games = self.replay_buffer.memory\n        states, actions, next_states, rewards, returns, dones = games\n        if self.critic.is_valid(): #This function returns False if the kernel hasn't be properly initialized, i.e x and fx haven't been set.\n            # We compute returns using the critic instead of MC returns.\n            returns = self.critic(np.concatenate([states,actions],axis=1))\n            games = states, actions, next_states, rewards, returns, dones\n        \n        self.critic = self.optimal_states_values_function(games,verbose=True,**kwargs)        \n        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KController\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class heuristic_ControllerCP:\n    \"\"\"\n    This class defines an expert-based heuristic controller for the CartPole environment.\n    \"\"\"\n    # This is the number of parameters to be optimized\n    dim = 4\n\n    def __init__(self, w=None, **kwargs):\n        if w is None:\n            self.w = np.ones([self.dim]) * 0.5\n        else:\n            self.w = w\n        pass\n\n    def get_distribution(self):\n        \"\"\"\n        This will be called by the optimizer. You need to define a way to sample from the parameters distribution, and get the support. \n        \"\"\"\n        class uniform:\n            def __init__(self, shape1):\n                self.shape1 = shape1\n\n            def __call__(self, n):\n                return 2 * np.random.uniform(size=[n, self.shape1]) - 1\n\n            def support(self, v):\n                return v\n\n        return uniform(self.w.shape[0])\n\n    def get_thetas(self):\n        return self.w\n\n    def set_thetas(self, w):\n        self.w = w.flatten()\n\n    def __call__(self, s, **kwargs):\n        \"\"\"\n        Will be used to make inference. This is where you define the action to be taken. \n\n        Parameters: \n        - s : state of the environment, a numpy array of shape (n, state_dim).\n\n        Returns: \n        - prod: int, action to be taken\n        \"\"\"\n        prod = (self.w * s).sum()\n        prod = int((np.sign(prod) + 1) / 2)\n        return prod\n    \nclass KControllerCP(KQLearning.KController):\n    \"\"\"\n    This is the main class which will optimize the heuristic controller. \n    \"\"\"\n    def __init__(self, state_dim, actions_dim, **kwargs):\n        # This is where you would pass any other custom controller\n        controller = heuristic_ControllerCP(state_dim=state_dim, **kwargs)\n        super().__init__(state_dim, actions_dim, controller, **kwargs)\n\n    def get_function(self, **kwargs):\n        \"\"\"\n        The optimizer will find the best parameters which maximizes this function. \n\n        This is where you would tweak the function to be maximized.\n        \"\"\"\n        self.expectation_estimator = self.get_expectation_estimator(self.x, self.y, **kwargs)\n        def function(x):\n            expectation = self.expectation_estimator(x)\n            distance = self.expectation_estimator.distance(x)\n            return expectation * distance\n        return function \n\n\n    def format(self, sarsd, **kwargs):\n        \"\"\"\n        In the case of the controller, the agent only sees the sum of the rewards for an entire episode. \n        All other game data won't be used for training. The format function still need to output a tuple. \n        \"\"\"\n        state, action, next_state, reward, done = [\n            core.get_matrix(e) for e in sarsd\n        ]\n        reward[done.astype(bool)] = 0\n\n        action = KQLearning.rl_hot_encoder(action, self.actions_dim)\n        action = core.get_matrix(self.controller.get_thetas()).T\n        done = core.get_matrix(done, dtype=bool)\n        return (\n            core.get_matrix(state.mean(axis=0)).T,\n            core.get_matrix(action.mean(axis=0)).T,\n            core.get_matrix(next_state.mean(axis=0)).T,\n            core.get_matrix(reward.sum(axis=0)).T,\n            core.get_matrix(done.mean(axis=0)).T,\n        )\n\n    def train(self, game, **kwargs):\n        # Similarily, you can skip training if the game is too long to save training time.\n        states, actions, next_states, rewards, dones = game\n        if len(states) >= kwargs.get(\"max_game\", 1e12):\n            print(\"no training\")\n            return\n        super().train(game, **kwargs)\n        \nif __name__ == \"__main__\":\n    # Define agents here, which will be trained in the benchmark. If game_dictionnary is empty, the benchmark will try to load data from the .pkl file\n    game_dictionary = {\n        \"PPOAgent\": PPOAgent,\n        \"PolicyGradient\": PolicyGradientCP,\n        \"Controller-based\": KControllerCP,\n        \"KACAgent\": KActorCriticCP,\n        \"DQNAgent\": DQNAgent,\n        \"KQLearningHJBCP\": KQLearningHJBCP,\n        \"KQLearning\": KQLearningCP,\n    }\n\n    # Define your agent's parameters here. This dict will be passed in each agent's __init__() method.\n    extras = {\n        # \"D\":4,\n        \"KActor\": {\"n_batch\": 1000000, \"max_nystrom\": 1000, \"reg\": 1e-9, \"order\": None},\n        \"KCritic\": {\n            \"n_batch\": 1000000,\n            \"max_nystrom\": 1000,\n            \"reg\": 1e-9,\n            \"order\": None,\n        },\n        \"Rewards\": {\n            \"n_batch\": 1000000,\n            \"max_nystrom\": 1000,\n            \"reg\": 1e-9,\n            \"order\": None,\n        },\n        \"DQNAgent\": {\n            # 'reward_function': mc_reward_function,\n            \"episodes\": 500,\n            \"policy_param\": 64,\n            \"target_param\": 64,\n        },\n        \"KController\": {\n            \"reg\": 1e-3,\n            \"order\": None,\n        },\n        \"HJBModel\": {\n            # \"latent_shape\":[100,50],\n            \"max_size\": 100000,\n            \"n_batch\": 1000000,\n            \"max_nystrom\": 1000,\n            \"reg\": 1e-9,\n            \"order\": None,\n            \"state_dim\": 4,\n        },\n        \"max_game\": 1000,\n        \"max_training_game_size\": 1000,\n        \"gamma\": 0.99,\n        \"capacity\": 200000000,\n        # \"seed\": 42,\n    }\n    seed = extras.get(\"seed\", None)\n    np.random.seed(seed)\n\n    Benchmark()(\n        game_dictionary,\n        \"CartPole-v1\",\n        num_games=100,\n        num_repeats=3,\n        max_time=3,\n        axis=\"episode\",\n        # file_name=\"results_CP_final.pkl\",\n        **extras,\n    )\n    plt.show()\n    pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
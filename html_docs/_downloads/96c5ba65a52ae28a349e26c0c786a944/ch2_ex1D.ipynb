{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 2.3 1D Periodic Function Extrapolation\nIn this experiment, we explore various machine learning and interpolation techniques\nto model and predict a sinusoidal function. We use different models, including CodPy,\nSciPy's RBF interpolator, Scikit-learn's SVR, Decision Trees, AdaBoost, Random Forest,\nFeedforward Neural Network, and XGBoost.\n\n## Objective\nThe goal of this experiment is to compare the performance of different models\nin predicting the output of a complex sinusoidal function (``periodic_fun``). The function\nis defined over a range of input values (``x``), and we generate predictions over\na broader range (``z``) to see how each model generalizes beyond the training data.\n\n## Data Generation\n\nWe define a sinusoidal function ``periodic_fun`` and use it to generate the target values (``fx``)\nfor a set of input values ``x`` ranging from -1 to 1. We generate a broader range of test inputs\n(``z``) ranging from -1.5 to 1.5 for evaluating the models.\n\n## Steps:\n\n1. Define a periodic sinusoidal function, ``periodic_fun``.\n2. Generate input data (``x``) and evaluate the function to obtain target values (``fx``).\n3. Generate test data (``z``) over a broader range to evaluate model predictions beyond the training data.\n4. Train different models using the input data (``x``, ``fx``) and predict values over the test range (``z``).\n5. Visualize the predictions from each model in a grid format for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Importing necessary modules\nimport os\nimport sys\nimport time\n\ncurr_f = os.path.join(os.getcwd(), \"codpy-book\", \"utils\")\nsys.path.insert(0, curr_f)\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# from codpy.plotting import plot1D\n# Lets import multi_plot function from codpy utils\nfrom codpy.plot_utils import multi_plot, plot1D\nfrom sklearn.metrics import mean_squared_error\n\n\n# Define the sinusoidal function\ndef periodic_fun(x):\n    \"\"\"\n    A sinusoidal function that generates a sum of sines based on the input ``x``.\n    \"\"\"\n    from math import pi\n\n    sinss = np.cos(2 * x * pi)\n    if x.ndim == 1:\n        sinss = np.prod(sinss, axis=0)\n        ress = np.sum(x, axis=0)\n    else:\n        sinss = np.prod(sinss, axis=1)\n        ress = np.sum(x, axis=1)\n    return ress + sinss\n\n\ndef periodicdata1D():\n    # lets define a simple 1D periodic data function\n    x = np.linspace(-1, 1, 100).reshape(-1, 1)\n    z = np.linspace(-1.5, 1.5, 100).reshape(-1, 1)\n    fx = np.array([periodic_fun(np.array([i])) for i in x])\n    fz = np.array([periodic_fun(np.array([i])) for i in z])\n    # Plot the data for x and z\n    multi_plot([(x, fx), (z, fz)], plot1D, mp_nrows=1, mp_figsize=(12, 3))\n\n\n# Call the function to generate and plot the data\nperiodicdata1D()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot shows a sinusoidal pattern for the data generated in the cartesian coordinate system.\nThe two curves for ``x`` and ``z`` exhibits the sinusoidal variations defined by ``periodic_fun(x)``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Model Setup and Training**\n\n We use several models, each wrapped as a function for modularity:\n\n - **CodPy Model**: Uses the CodPy library's kernel regression model with a specified kernel.\n - **RBF Interpolator (SciPy)**: A radial basis function interpolator that uses a multiquadric kernel.\n - **SVR (Scikit-learn)**: Support Vector Regressor using a radial basis function (RBF) kernel.\n - **Neural Network (Pytorch)**: A simple feedforward neural network with two hidden layers.\n - **Decision Tree (Scikit-learn)**: A decision tree regressor with a maximum depth of 10.\n - **AdaBoost (Scikit-learn)**: An AdaBoost model with a decision tree as the base estimator.\n - **XGBoost**: A gradient-boosted tree model from the XGBoost library.\n - **Random Forest (Scikit-learn)**: An ensemble of decision trees trained with random subsets of the data.\n\n### Prediction\n\n Each model is trained on the generated dataset (``x``, ``fx``) and then used to predict the values over\n the test range (``z``). The predictions (``fz``) are stored and transformed into a compatible format for plotting.\n\n### Expected Output\n\n The output consists of a 2x4 grid of plots, where each plot displays the predictions from one of the models\n compared to the underlying function. The aim is to observe differences in accuracy and generalization ability\n between models, especially beyond the training range.\n\n########################################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First we import necessary libraries\n# import CodPy's core module and Kernel class\nfrom codpy import core\nfrom codpy.kernel import Kernel\nfrom scipy.interpolate import Rbf\nfrom sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\n\n# Model functions\n# 0. CodPy\ndef codpy_model(x, fx, z):\n    kernel = Kernel(\n        set_kernel=core.kernel_setter(\"gaussianper\", None,2,1e-9), x=x, fx=fx, order=2\n    )\n    res = kernel(z)\n    mmd = np.sqrt(kernel.discrepancy(z))\n    return res, mmd\n\n\n# 1. SciPy RBF Interpolator\ndef rbf_interpolator(x, fx, z):\n    rbf = Rbf(x.ravel(), fx.ravel(), function=\"multiquadric\")\n    return rbf(z.ravel()), None\n\n\n# 2. Scikit-learn SVR\ndef svr_model(x, fx, z):\n    svr = SVR(kernel=\"rbf\", gamma=\"auto\", C=1)\n    svr.fit(x, fx.ravel())\n    return svr.predict(z), None\n\n\n# 3. NN\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n\nclass FFN(nn.Module):\n    def __init__(\n        self, in_features, hidden_features=64, hidden_layers=3, out_features=1\n    ):\n        super().__init__()\n        layers = []\n        layers.append(nn.Linear(in_features, hidden_features))\n        layers.append(nn.ReLU())\n        for _ in range(hidden_layers - 1):\n            layers.append(nn.Linear(hidden_features, hidden_features))\n            layers.append(nn.ReLU())\n        layers.append(nn.Linear(hidden_features, out_features))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndef neural_network(x, fx, z):\n    x_tensor = torch.tensor(x, dtype=torch.float32)\n    fx_tensor = torch.tensor(fx, dtype=torch.float32).view(-1, 1)\n    z_tensor = torch.tensor(z, dtype=torch.float32)\n\n    dataset = TensorDataset(x_tensor, fx_tensor)\n    val_size = int(0.1 * len(dataset))\n    train_size = len(dataset) - val_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n\n    model = FFN(in_features=1, hidden_features=64, hidden_layers=3, out_features=1)\n\n    # loss and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    model.train()\n    for epoch in range(150):\n        for batch_x, batch_y in train_loader:\n            optimizer.zero_grad()\n            outputs = model(batch_x)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        predictions = model(z_tensor).flatten().numpy()\n\n    return predictions, None\n\n\n# 4. Decision Tree Regressor\ndef decision_tree(x, fx, z):\n    tree = DecisionTreeRegressor(max_depth=10)\n    tree.fit(x, fx.ravel())\n    return tree.predict(z), None\n\n\n# 5. AdaBoost Regressor\ndef adaboost_model(x, fx, z):\n    ada = AdaBoostRegressor(\n        DecisionTreeRegressor(max_depth=5), n_estimators=50, learning_rate=1\n    )\n    ada.fit(x, fx.ravel())\n    return ada.predict(z), None\n\n\n# 6. XGBoost Regressor\ndef xgboost_model(x, fx, z):\n    xgb = XGBRegressor(max_depth=5, n_estimators=10)\n    xgb.fit(x, fx.ravel())\n    return xgb.predict(z), None\n\n\n# 7. Random Forest Regressor\ndef random_forest(x, fx, z):\n    rf = RandomForestRegressor(max_depth=5, n_estimators=5)\n    rf.fit(x, fx.ravel())\n    return rf.predict(z), None\n\n\n# List of model functions\nmodel_functions = [\n    codpy_model,\n    rbf_interpolator,\n    svr_model,\n    neural_network,\n    decision_tree,\n    adaboost_model,\n    xgboost_model,\n    random_forest,\n]\n\n\ndef plot_models():\n    \"\"\"\n    This function generates random data for x and z coordinates, applies each model\n    in `model_functions` to generate predictions, and plots the results using `multi_plot`.\n    \"\"\"\n\n    # Generate x and z data\n    x = np.linspace(-1, 1, 100).reshape(-1, 1)\n    z = np.linspace(-1.5, 1.5, 100).reshape(-1, 1)\n\n    # Apply the periodic function to generate fx and fz values\n    fx = np.array([periodic_fun(np.array([i])) for i in x])\n    fz = np.array([periodic_fun(np.array([i])) for i in z])\n\n    # Generate predictions for each model in the model_functions list\n    list_of_results = [model(x, fx, z) for model in model_functions]\n\n    # Titles for each subplot\n    title_list = [\n        \"CodPy\",\n        \"RBF (SciPy)\",\n        \"SVR (Scikit)\",\n        \"FFN\",\n        \"Decision Tree (Scikit)\",\n        \"AdaBoost (Scikit)\",\n        \"XGBoost\",\n        \"Random Forest (Scikit)\",\n    ]\n\n    # Use a lambda function to transform z and fz into (z, fz) tuples\n    zfz_transform = lambda z, fz: (z.ravel(), fz.ravel())\n\n    # Apply the transformation to each model's output\n    transformed_results = [zfz_transform(z, result) for result, _ in list_of_results]\n\n    # Create the multi-plot visualization\n    multi_plot(\n        transformed_results,\n        plot1D,\n        f_names=title_list,\n        mp_max_items=8,\n        mp_nrows=2,\n        mp_ncols=4,\n        mp_figsize=(16, 8),\n    )\n    plt.show()\n\n    Nxs = np.arange(start=10, stop=len(x) + 1, step=(len(x) - 10) // 5)\n    results = {}\n    times = {}\n    mmds = {}\n    for Nx in Nxs:\n        results[Nx] = {}\n        times[Nx] = {}\n        mmds[Nx] = {}\n        for i, model in enumerate(model_functions):\n            start = time.perf_counter()\n            f_z, mmd = model(x[:Nx], fx[:Nx], z)\n            end = time.perf_counter()\n            mmds[Nx][title_list[i]] = mmd\n            results[Nx][title_list[i]] = mean_squared_error(fz, f_z)\n            times[Nx][title_list[i]] = end - start\n            print(f\"Model: {title_list[i]}, Time taken: {end-start:.4f} seconds\")\n\n    res = [{\"data\": results}, {\"data\": mmds}, {\"data\": times}]\n\n    def plot_one(inputs):\n        results = inputs[\"data\"]\n        ax = inputs[\"ax\"]\n        legend = inputs[\"legend\"]\n        for model_name in next(iter(results.values())).keys():\n            x_vals = sorted(results.keys())\n            y_vals = [results[x][model_name] for x in x_vals]\n            ax.plot(x_vals, y_vals, marker=\"o\", label=model_name)\n        ax.set_xlabel(\"Nx\")\n        ax.set_ylabel(legend)\n        ax.legend()\n        ax.grid(True)\n\n    multi_plot(\n        res,\n        plot_one,\n        mp_nrows=1,\n        mp_ncols=3,\n        legends=[\"Scores\", \"Discrepancy\", \"Times\"],\n        mp_figsize=(14, 10),\n    )\n    plt.show()\n\n\n# Lets plot the models\nplot_models()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}